"""
PDF Parser - –ò–°–ü–†–ê–í–õ–ï–ù–û
–¢–µ–ø–µ—Ä—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç PDF —Ç–∞–±–ª–∏—Ü—ã –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç positions[]
"""
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
import pdfplumber
import re

from app.utils.position_normalizer import normalize_positions

logger = logging.getLogger(__name__)


class PDFParser:
    """Parse construction estimates from PDF files"""
    
    def parse(self, file_path: Path) -> Dict[str, Any]:
        """
        Parse PDF file and extract positions
        
        Args:
            file_path: Path to PDF file
            
        Returns:
            {
                "document_info": {...},
                "positions": [...]
            }
        """
        logger.info(f"üìÑ Parsing PDF: {file_path.name}")
        
        try:
            positions = []
            
            with pdfplumber.open(file_path) as pdf:
                total_pages = len(pdf.pages)
                logger.info(f"PDF has {total_pages} pages")
                
                # Extract tables from all pages
                for page_num, page in enumerate(pdf.pages, start=1):
                    tables = page.extract_tables()
                    
                    if tables:
                        logger.info(f"Found {len(tables)} table(s) on page {page_num}")
                        
                        for table_idx, table in enumerate(tables, start=1):
                            logger.debug(
                                f"Processing table {table_idx} on page {page_num} "
                                f"({len(table)} rows)"
                            )
                            
                            # Convert table to positions
                            table_positions = self._table_to_positions(
                                table, 
                                page_num, 
                                table_idx
                            )
                            
                            positions.extend(table_positions)
                            logger.info(
                                f"Extracted {len(table_positions)} positions "
                                f"from table {table_idx} on page {page_num}"
                            )
            
            logger.info(f"Total raw positions extracted: {len(positions)}")
            
            # Normalize all positions
            normalized_positions = normalize_positions(positions)
            
            logger.info(
                f"‚úÖ PDF parsed: {len(normalized_positions)} valid positions "
                f"from {total_pages} pages"
            )
            
            return {
                "document_info": {
                    "filename": file_path.name,
                    "format": "pdf",
                    "total_pages": total_pages,
                    "tables_found": len(positions)
                },
                "positions": normalized_positions
            }
            
        except Exception as e:
            logger.error(f"‚ùå PDF parsing failed: {str(e)}", exc_info=True)
            return {
                "document_info": {
                    "filename": file_path.name,
                    "format": "pdf",
                    "error": str(e)
                },
                "positions": []
            }
    
    def _table_to_positions(
        self, 
        table: List[List[str]], 
        page_num: int,
        table_idx: int
    ) -> List[Dict[str, Any]]:
        """
        Convert PDF table to list of position dictionaries
        
        Args:
            table: List of table rows (each row is list of cells)
            page_num: Page number for reference
            table_idx: Table index on the page
            
        Returns:
            List of raw position dicts (not normalized yet)
        """
        if not table or len(table) < 2:
            logger.debug("Table too small or empty")
            return []
        
        # First row is usually header
        headers = table[0]
        
        if not headers:
            logger.debug("No headers found in table")
            return []
        
        # Clean and normalize headers
        clean_headers = []
        for h in headers:
            if h:
                # Clean header: lowercase, remove special chars
                clean = str(h).lower().strip()
                clean = re.sub(r'[^\w\s]', '', clean)
                clean = re.sub(r'\s+', '_', clean)
                clean_headers.append(clean)
            else:
                clean_headers.append(f"col_{len(clean_headers)}")
        
        logger.debug(f"Table headers: {clean_headers}")
        
        positions = []
        
        # Process data rows
        for row_idx, row in enumerate(table[1:], start=1):
            if not row or all(not cell for cell in row):
                # Skip empty rows
                continue
            
            # Create position dict from row
            position = {}
            for col_idx, cell in enumerate(row):
                if col_idx < len(clean_headers):
                    header = clean_headers[col_idx]
                    if cell and str(cell).strip():
                        position[header] = str(cell).strip()
            
            # Skip rows that look like sub-headers or separators
            if self._is_separator_row(position):
                continue
            
            # Add metadata
            position['_source'] = f"page_{page_num}_table_{table_idx}_row_{row_idx}"
            
            if position:
                positions.append(position)
        
        logger.debug(f"Converted table to {len(positions)} raw positions")
        
        return positions
    
    @staticmethod
    def _is_separator_row(position: Dict[str, Any]) -> bool:
        """
        Check if row is a separator or sub-header
        
        Returns True if row should be skipped
        """
        if not position:
            return True
        
        # Check if all values are short (likely header/separator)
        all_values = ' '.join(str(v) for v in position.values())
        
        # Separators often have repeated characters
        if re.match(r'^[\-\=\*\s]+$', all_values):
            return True
        
        # Very short rows (< 5 chars) are likely not positions
        if len(all_values) < 5:
            return True
        
        return False
    
    def get_supported_extensions(self) -> set:
        """Return supported file extensions"""
        return {'.pdf'}


# ==============================================================================
# USAGE EXAMPLE
# ==============================================================================

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        pdf_path = Path(sys.argv[1])
        
        parser = PDFParser()
        result = parser.parse(pdf_path)
        
        print(f"\nüìä Parsing Results:")
        print(f"File: {result['document_info']['filename']}")
        print(f"Pages: {result['document_info'].get('total_pages', 0)}")
        print(f"Positions found: {len(result['positions'])}")
        
        if result['positions']:
            print(f"\nüìù First 3 positions:")
            for pos in result['positions'][:3]:
                print(f"\n  Position: {pos.get('position_number', 'N/A')}")
                print(f"  Description: {pos.get('description', 'N/A')[:50]}")
                print(f"  Quantity: {pos.get('quantity', 0)} {pos.get('unit', '')}")
                if 'unit_price' in pos:
                    print(f"  Price: {pos['unit_price']} CZK")
    else:
        print("Usage: python pdf_parser.py <path_to_pdf>")
        print("Example: python pdf_parser.py data/raw/project1/vykresy/estimate.pdf")
