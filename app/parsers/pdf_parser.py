"""
Enhanced PDF Parser with Smart Text Detection
–ë–ï–ó CLAUDE FALLBACK - —Ç–æ–ª—å–∫–æ pdfplumber

‚úÖ –ù–û–í–û–ï: Smart Text Detection - –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
‚úÖ –ü—Ä–æ–ø—É—Å–∫ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü - —ç–∫–æ–Ω–æ–º–∏—è –≤—Ä–µ–º–µ–Ω–∏
‚úÖ –î–µ—Ç–µ–∫—Ü–∏—è —Å–∫–∞–Ω–æ–≤ - –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±—É–¥—É—â–µ–≥–æ OCR
‚úÖ Page-by-page processing - memory efficient
"""
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
import re

logger = logging.getLogger(__name__)


class PDFParser:
    """
    Enhanced PDF parser –¥–ª—è —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–º–µ—Ç
    
    ‚úÖ –ë–ï–ó Claude fallback - —Ç–æ–ª—å–∫–æ pdfplumber
    ‚úÖ Smart Text Detection - –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    ‚úÖ Page-by-page processing (memory efficient)
    ‚úÖ –ß–µ—à—Å–∫–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ (MJ, J.cena, –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞)
    """
    
    def __init__(self):
        """–ë–ï–ó claude_client –∏ mineru_client!"""
        pass
    
    def parse(self, pdf_path: Path, max_pages: int = 100) -> Dict[str, Any]:
        """
        Parse PDF estimate with Smart Detection
        
        Args:
            pdf_path: Path to PDF file
            max_pages: Maximum pages to process
            
        Returns:
            Dict with parsed data
        """
        logger.info(f"üìÑ Parsing PDF with Smart Detection: {pdf_path}")
        
        return self._parse_with_smart_detection(pdf_path, max_pages)
    
    def _parse_with_smart_detection(self, pdf_path: Path, max_pages: int = 100) -> Dict[str, Any]:
        """
        Parse PDF using pdfplumber with Smart Page Detection
        
        ‚úÖ –ù–û–í–û–ï: –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (text/image/empty)
        """
        try:
            import pdfplumber
        except ImportError:
            raise ImportError("pdfplumber not installed. Install with: pip install pdfplumber")
        
        logger.info("Using pdfplumber with Smart Detection...")
        
        positions = []
        page_info = []
        all_text = []
        
        with pdfplumber.open(pdf_path) as pdf:
            total_pages = len(pdf.pages)
            pages_to_process = min(total_pages, max_pages)
            
            logger.info(f"Processing {pages_to_process} of {total_pages} pages")
            
            for page_num in range(pages_to_process):
                page = pdf.pages[page_num]
                
                # ‚úÖ –ù–û–í–û–ï: Smart Detection - –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                page_type, confidence = self._detect_page_type(page)
                
                page_info.append({
                    'page_number': page_num + 1,
                    'type': page_type,
                    'confidence': confidence
                })
                
                logger.info(f"  Page {page_num+1}/{pages_to_process}: {page_type} (confidence: {confidence:.0%})")
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
                if page_type == 'text':
                    # ‚úÖ –¢–µ–∫—Å—Ç–æ–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ - –æ–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
                    
                    # –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                    text = page.extract_text()
                    if text:
                        all_text.append(f"=== Page {page_num + 1} ===\n{text}")
                    
                    # –ò–∑–≤–ª–µ—á—å —Ç–∞–±–ª–∏—Ü—ã
                    tables = page.extract_tables()
                    for table_num, table in enumerate(tables, 1):
                        logger.info(f"    Found table {table_num} on page {page_num + 1}")
                        
                        # Parse table as positions
                        parsed_positions = self._parse_table(table, page_num + 1, table_num)
                        positions.extend(parsed_positions)
                
                elif page_type == 'empty':
                    # ‚è≠Ô∏è –ü—É—Å—Ç–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ - –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å
                    logger.info(f"    ‚è≠Ô∏è  Skipping empty page")
                
                elif page_type == 'image':
                    # üñºÔ∏è –°–∫–∞–Ω/–∫–∞—Ä—Ç–∏–Ω–∫–∞ - –ø–æ–∫–∞ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–∏–ª–∏ OCR –µ—Å–ª–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω)
                    logger.warning(f"    üñºÔ∏è  Page is a scan/image (OCR not implemented)")
                    logger.warning(f"    ‚ö†Ô∏è  To process scans, consider adding EasyOCR integration")
                
                # ‚úÖ Explicitly release page memory
                del page
        
        full_text = "\n\n".join(all_text)
        
        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–∏–ø–∞–º —Å—Ç—Ä–∞–Ω–∏—Ü
        page_type_summary = self._summarize_page_types(page_info)
        
        result = {
            "positions": positions,
            "total_positions": len(positions),
            "document_info": {
                "document_type": "PDF Estimate",
                "format": "PDF_SMART_DETECTION",
                "pages": total_pages,
                "pages_processed": pages_to_process,
                "page_types": page_type_summary
            },
            "page_info": page_info,  # ‚úÖ –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            "sections": [],
            "raw_text": full_text[:5000]  # First 5000 chars for reference
        }
        
        logger.info(f"‚úÖ Extracted {len(positions)} positions from PDF")
        logger.info(f"   Page types: {page_type_summary}")
        
        return result
    
    def _detect_page_type(self, page) -> tuple[str, float]:
        """
        ‚úÖ –ù–û–í–û–ï: Smart Detection - –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç:
        - 'text' - –∂–∏–≤–æ–π —Ç–µ–∫—Å—Ç (–º–æ–∂–Ω–æ –∏–∑–≤–ª–µ–∫–∞—Ç—å)
        - 'image' - —Å–∫–∞–Ω/–∫–∞—Ä—Ç–∏–Ω–∫–∞ (–Ω—É–∂–µ–Ω OCR)
        - 'empty' - –ø—É—Å—Ç–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        
        Args:
            page: pdfplumber page object
            
        Returns:
            (type, confidence) - —Ç–∏–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (0.0-1.0)
        """
        # –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç
        text = page.extract_text()
        
        # === –ü–†–û–í–ï–†–ö–ê 1: –°–æ–≤—Å–µ–º –Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞ ===
        if not text or len(text.strip()) < 5:
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –µ—Å—Ç—å –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if self._has_images(page):
                return ('image', 0.9)
            return ('empty', 0.95)
        
        # === –ü–†–û–í–ï–†–ö–ê 2: –ú–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ ===
        text_clean = text.strip()
        words = text_clean.split()
        
        if len(words) < 3:
            # –û—á–µ–Ω—å –º–∞–ª–æ —Å–ª–æ–≤ - —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –ø—É—Å—Ç–∞—è –∏–ª–∏ –∫–∞—Ä—Ç–∏–Ω–∫–∞ —Å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞–º–∏
            if self._has_images(page):
                return ('image', 0.85)
            return ('empty', 0.8)
        
        # === –ü–†–û–í–ï–†–ö–ê 3: –ü—Ä–æ—Ü–µ–Ω—Ç –±—É–∫–≤ (vs –º—É—Å–æ—Ä) ===
        alpha_chars = sum(c.isalpha() for c in text)
        digit_chars = sum(c.isdigit() for c in text)
        total_chars = len(text.replace(' ', '').replace('\n', '').replace('\t', ''))
        
        if total_chars == 0:
            return ('empty', 0.9)
        
        alpha_ratio = alpha_chars / total_chars
        meaningful_ratio = (alpha_chars + digit_chars) / total_chars
        
        # –ï—Å–ª–∏ –º–µ–Ω—å—à–µ 5% –±—É–∫–≤ - —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –º—É—Å–æ—Ä –∏–ª–∏ –ø–ª–æ—Ö–æ–π OCR
        if alpha_ratio < 0.05:
            if self._has_images(page):
                return ('image', 0.8)
            return ('empty', 0.7)
        
        # === –ü–†–û–í–ï–†–ö–ê 4: –ï—Å—Ç—å –ª–∏ —Ç–∞–±–ª–∏—Ü—ã? ===
        tables = page.extract_tables()
        if tables and len(tables) > 0:
            # –¢–æ—á–Ω–æ –∂–∏–≤–æ–π —Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–∞–±–ª–∏—Ü—ã!
            return ('text', 0.95)
        
        # === –ü–†–û–í–ï–†–ö–ê 5: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Å–º–µ—Ç ===
        text_lower = text.lower()
        
        # –ß–µ—à—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        czech_keywords = [
            'k√≥d', 'kod', 'popis', 'n√°zev', 'nazev',
            'mno≈æstv√≠', 'mnozstvi', 'cena', 'celkem',
            'stavba', 'objekt', 'pr√°ce', 'prace',
            'materi√°l', 'material', 'jednotka', 'mj'
        ]
        
        # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        english_keywords = [
            'code', 'description', 'quantity', 'unit',
            'price', 'total', 'amount', 'work', 'item'
        ]
        
        keywords_found = sum(1 for kw in (czech_keywords + english_keywords) if kw in text_lower)
        
        if keywords_found >= 3:
            # –ú–Ω–æ–≥–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ - —Ç–æ—á–Ω–æ –∂–∏–≤–æ–π —Ç–µ–∫—Å—Ç!
            return ('text', 0.9)
        
        if keywords_found >= 1:
            # –ï—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
            return ('text', 0.7)
        
        # === –ü–†–û–í–ï–†–ö–ê 6: –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ ===
        if len(words) > 50 and meaningful_ratio > 0.3:
            # –ú–Ω–æ–≥–æ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            return ('text', 0.8)
        
        if len(words) > 20 and meaningful_ratio > 0.2:
            # –°—Ä–µ–¥–Ω–µ —Ç–µ–∫—Å—Ç–∞
            return ('text', 0.6)
        
        # === –ü–†–û–í–ï–†–ö–ê 7: –ï—Å—Ç—å –ª–∏ –±–æ–ª—å—à–∏–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏? ===
        if self._has_large_images(page):
            return ('image', 0.75)
        
        # === –ü–û –£–ú–û–õ–ß–ê–ù–ò–Æ ===
        # –ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω—ã –Ω–æ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç - —Å—á–∏—Ç–∞–µ–º —á—Ç–æ —Ç–µ–∫—Å—Ç
        if len(words) > 5:
            return ('text', 0.5)
        
        # –°–æ–≤—Å–µ–º –Ω–µ —É–≤–µ—Ä–µ–Ω—ã - empty
        return ('empty', 0.5)
    
    def _has_images(self, page) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –µ—Å—Ç—å –ª–∏ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
        try:
            images = page.images
            return len(images) > 0
        except Exception as e:
            logger.debug(f"Failed to check images: {e}")
            return False
    
    def _has_large_images(self, page) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –µ—Å—Ç—å –ª–∏ –±–æ–ª—å—à–∏–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ (>50% —Å—Ç—Ä–∞–Ω–∏—Ü—ã)"""
        try:
            images = page.images
            if not images:
                return False
            
            # –†–∞–∑–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_width = page.width
            page_height = page.height
            page_area = page_width * page_height
            
            # –°—É–º–º–∞—Ä–Ω–∞—è –ø–ª–æ—â–∞–¥—å –∫–∞—Ä—Ç–∏–Ω–æ–∫
            images_area = 0
            for img in images:
                img_width = img.get('width', 0)
                img_height = img.get('height', 0)
                images_area += img_width * img_height
            
            # –ï—Å–ª–∏ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∑–∞–Ω–∏–º–∞—é—Ç >50% —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            return images_area > (page_area * 0.5)
        
        except Exception as e:
            logger.debug(f"Failed to check large images: {e}")
            return False
    
    def _summarize_page_types(self, page_info: List[Dict]) -> Dict[str, int]:
        """
        –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–∏–ø–∞–º —Å—Ç—Ä–∞–Ω–∏—Ü
        
        Args:
            page_info: List of page info dicts
            
        Returns:
            Summary dict with counts
        """
        summary = {
            'text': 0,
            'image': 0,
            'empty': 0,
            'total': len(page_info)
        }
        
        for page in page_info:
            page_type = page.get('type', 'text')
            summary[page_type] = summary.get(page_type, 0) + 1
        
        # –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ—Ü–µ–Ω—Ç—ã
        if summary['total'] > 0:
            summary['text_pct'] = round(summary['text'] / summary['total'] * 100, 1)
            summary['image_pct'] = round(summary['image'] / summary['total'] * 100, 1)
            summary['empty_pct'] = round(summary['empty'] / summary['total'] * 100, 1)
        
        return summary
    
    def _parse_table(self, table: List[List[str]], page_num: int, table_num: int) -> List[Dict[str, Any]]:
        """
        Parse a table extracted from PDF
        
        Args:
            table: 2D list of table cells
            page_num: Page number
            table_num: Table number on page
            
        Returns:
            List of parsed positions
        """
        if not table or len(table) < 2:
            return []
        
        positions = []
        
        # Try to identify header row
        header = table[0]
        header_lower = [str(cell).lower().strip() if cell else '' for cell in header]
        
        # Find column indices - ‚úÖ –†–ê–°–®–ò–†–ï–ù–ù–´–ô –ü–û–ò–°–ö (—á–µ—à—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã)
        code_idx = self._find_column(header_lower, [
            'code', 'k√≥d', 'kod', 'ƒç√≠slo', 'cislo', 'polo≈æka', 'polozka'
        ])
        desc_idx = self._find_column(header_lower, [
            'description', 'popis', 'n√°zev', 'nazev', 'name', 'text'
        ])
        unit_idx = self._find_column(header_lower, [
            'unit', 'mj', 'm.j.', 'jednotka', 'j.'
        ])
        qty_idx = self._find_column(header_lower, [
            'quantity', 'mno≈æstv√≠', 'mnozstvi', 'poƒçet', 'pocet', 'qty'
        ])
        price_idx = self._find_column(header_lower, [
            'price', 'cena', 'j.cena', 'j. cena', 'jednotkov√°', 'jednotkova', 'unit price'
        ])
        total_idx = self._find_column(header_lower, [
            'celkem', 'total', 'cena celkem', 'total price', 'amount'
        ])
        
        logger.debug(f"    Column mapping: desc={desc_idx}, qty={qty_idx}, unit={unit_idx}, price={price_idx}")
        
        # Parse data rows
        for row_idx, row in enumerate(table[1:], 1):
            if not row or len(row) == 0:
                continue
            
            # Skip rows that look like headers or totals
            if self._is_header_or_total_row(row):
                continue
            
            # Get description (required)
            description = self._get_cell(row, desc_idx, '')
            if not description or len(description.strip()) < 3:
                continue
            
            position = {
                "code": self._get_cell(row, code_idx, ''),
                "description": description,
                "unit": self._get_cell(row, unit_idx, ''),
                "quantity": self._parse_float(self._get_cell(row, qty_idx, '0')),
                "unit_price": self._parse_float(self._get_cell(row, price_idx, '0')),
                "total_price": self._parse_float(self._get_cell(row, total_idx, '0')),
                "page": page_num,
                "table": table_num,
                "row": row_idx
            }
            
            # Calculate total if missing
            if position["quantity"] and position["unit_price"] and not position["total_price"]:
                position["total_price"] = position["quantity"] * position["unit_price"]
            
            # Only add if we have meaningful data
            if position["description"]:
                positions.append(position)
        
        return positions
    
    def _find_column(self, header: List[str], keywords: List[str]) -> Optional[int]:
        """
        Find column index by keywords
        ‚úÖ CASE-INSENSITIVE –ø–æ–∏—Å–∫
        """
        for idx, cell in enumerate(header):
            cell_clean = cell.lower().strip()
            for keyword in keywords:
                if keyword in cell_clean:
                    return idx
        return None
    
    def _get_cell(self, row: List, idx: Optional[int], default: str = '') -> str:
        """Safely get cell value"""
        if idx is None or idx >= len(row):
            return default
        return str(row[idx]).strip() if row[idx] else default
    
    def _is_header_or_total_row(self, row: List) -> bool:
        """Check if row is a header or total row"""
        row_text = ' '.join([str(cell).lower() for cell in row if cell]).strip()
        
        skip_keywords = [
            'total', 'celkem', 'suma', 'subtotal', 'mezisouƒçet',
            'code', 'k√≥d', 'popis', 'description', 'quantity', 'mno≈æstv√≠'
        ]
        
        # If row text is short and matches keyword
        if len(row_text) < 30:
            return any(keyword in row_text for keyword in skip_keywords)
        
        return False
    
    def _parse_float(self, text: str) -> float:
        """
        Parse float from text
        ‚úÖ –ß–µ—à—Å–∫–∏–µ —á–∏—Å–ª–∞ (1 220,168)
        """
        try:
            text = str(text).strip()
            
            # Remove spaces (thousands separators)
            text = text.replace(' ', '').replace('\xa0', '')
            
            # Remove currency symbols
            text = text.replace('Kƒç', '').replace('CZK', '').replace('EUR', '').replace('‚Ç¨', '')
            text = text.strip()
            
            # Handle decimal separator
            # If both . and , ‚Üí European format (1.220,50)
            if '.' in text and ',' in text:
                # European: dot = thousands, comma = decimal
                text = text.replace('.', '')
                text = text.replace(',', '.')
            elif ',' in text:
                # Only comma - check if decimal or thousands
                parts = text.split(',')
                if len(parts) == 2 and len(parts[1]) <= 2:
                    # Decimal: 15,50
                    text = text.replace(',', '.')
                else:
                    # Thousands: 1,220
                    text = text.replace(',', '')
            
            # Clean any remaining non-numeric
            text = re.sub(r'[^\d.-]', '', text)
            
            if not text or text == '-':
                return 0.0
            
            return float(text)
        except (ValueError, AttributeError):
            return 0.0
