# agents/concrete_agent_hybrid.py
"""
–ì–∏–±—Ä–∏–¥–Ω–∞—è –≤–µ—Ä—Å–∏—è –∞–≥–µ–Ω—Ç–∞: –±—ã—Å—Ç—Ä—ã–π + —Ç–æ—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ª—É—á—à–µ–µ –∏–∑ –æ–±–µ–∏—Ö –≤–µ—Ä—Å–∏–π
"""
import json
import re
import logging
import os
import sys
import asyncio
from typing import List, Dict, Any, Optional

# –ò–º–ø–æ—Ä—Ç—ã
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from parsers.doc_parser import DocParser
    from parsers.excel_parser import ExcelParser
    from parsers.xml_smeta_parser import XMLSmetaParser
    from utils.claude_client import get_claude_client
    PARSERS_AVAILABLE = True
except ImportError as e:
    logging.warning(f"Import warning: {e}")
    PARSERS_AVAILABLE = False

logger = logging.getLogger(__name__)

class HybridConcreteAnalysisAgent:
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–≥–µ–Ω—Ç: —Å–æ—á–µ—Ç–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –ø—Ä–æ—Å—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å –º–æ—â—å—é Claude
    """
    
    def __init__(self, use_claude: bool = True, claude_mode: str = "enhancement"):
        """
        Args:
            use_claude: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ Claude API
            claude_mode: "enhancement" | "primary" | "fallback"
        """
        self.use_claude = use_claude
        self.claude_mode = claude_mode
        self.claude_client = get_claude_client() if use_claude else None
        
        if not PARSERS_AVAILABLE:
            raise ImportError("–ü–∞—Ä—Å–µ—Ä—ã –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã")
            
        self.doc_parser = DocParser()
        self.excel_parser = ExcelParser()
        self.xml_parser = XMLSmetaParser()
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–∏–∑ –æ–±–µ–∏—Ö –≤–µ—Ä—Å–∏–π)
        self.concrete_patterns = [
            r'\b(C\d{1,2}/\d{1,2})\b',              # C25/30, C30/37
            r'\b(B\d{1,2})\b',                      # B20, B25, B30
            r'(?:beton|bet√≥n)\s+([BCM]\d+(?:/\d+)?)', # beton C25/30
            r'(?:t≈ô√≠da|t≈ô√≠da betonu)\s+([BCM]\d+(?:/\d+)?)'
        ]
        
        # –ö–ª–∞—Å—Å—ã —Å—Ä–µ–¥—ã (–∏–∑ –≤–µ—Ä—Å–∏–∏ 2)
        self.env_classes_regex = r'\b(XO|XC[1-4]|XD[1-3]|XS[1-3]|XF[1-4]|XA[1-3])\b'
        self.workability_regex = r'\b(S[1-5])\b'
        
        # –û–ø–∏—Å–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ —Å—Ä–µ–¥—ã
        self.env_class_descriptions = {
            "XO": "Bez rizika koroze. Pro beton bez v√Ωztu≈æe v such√©m prost≈ôed√≠.",
            "XC1": "Koroze karbonatac√≠ ‚Äì such√© nebo trvale vlhk√© prost≈ôed√≠.",
            "XC2": "Koroze karbonatac√≠ ‚Äì vlhk√©, obƒças such√© prost≈ôed√≠.",
            "XC3": "Koroze karbonatac√≠ ‚Äì st≈ôednƒõ vlhk√© prost≈ôed√≠.",
            "XC4": "Koroze karbonatac√≠ ‚Äì st≈ô√≠davƒõ vlhk√© a such√© prost≈ôed√≠.",
            "XD1": "Koroze chloridy ‚Äì st≈ôednƒõ vlhk√© prost≈ôed√≠.",
            "XD2": "Koroze chloridy ‚Äì vlhk√©, obƒças such√© prost≈ôed√≠.",
            "XD3": "Koroze chloridy ‚Äì st≈ô√≠davƒõ vlhk√© a such√© prost≈ôed√≠.",
            "XS1": "Mo≈ôsk√° s≈Øl ve vzduchu ‚Äì povrch vystaven√Ω sol√≠m.",
            "XS2": "St√°l√© pono≈ôen√≠ do mo≈ôsk√© vody.",
            "XS3": "Z√≥ny p≈ô√≠livu, odlivu a rozst≈ôik≈Ø mo≈ôsk√© vody.",
            "XF1": "Zamrz√°n√≠/rozmrazov√°n√≠ ‚Äì m√≠rnƒõ nas√°kav√©, bez posypov√Ωch sol√≠.",
            "XF2": "Zamrz√°n√≠/rozmrazov√°n√≠ ‚Äì m√≠rnƒõ nas√°kav√©, s posypov√Ωmi solemi.",
            "XF3": "Zamrz√°n√≠/rozmrazov√°n√≠ ‚Äì silnƒõ nas√°kav√©, bez posypov√Ωch sol√≠.",
            "XF4": "Zamrz√°n√≠/rozmrazov√°n√≠ ‚Äì silnƒõ nas√°kav√©, s posypov√Ωmi solemi.",
            "XA1": "Slabƒõ agresivn√≠ chemick√© prost≈ôed√≠.",
            "XA2": "St≈ôednƒõ agresivn√≠ chemick√© prost≈ôed√≠.",
            "XA3": "Silnƒõ agresivn√≠ chemick√© prost≈ôed√≠."
        }
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ)
        self.context_patterns = {
            "z√°klady": r'z√°klad|foundation',
            "vƒõnce": r'vƒõnec|ring beam',
            "piloty": r'pilot[ay]?|pile',
            "stƒõny": r'p≈ô√≠ƒçka|zeƒè|stƒõna|wall',
            "sloupy": r'sloup|pil√≠≈ô|column',
            "pr≈Øvlaky": r'pr≈Øvlak|beam',
            "deska": r'deska|slab|floor',
            "schodi≈°tƒõ": r'schodi≈°tƒõ|stair',
            "st≈ôecha": r'st≈ôecha|roof',
            "mostn√≠ konstrukce": r'most|bridge',
            "spodn√≠ stavba": r'spodn√≠ stavba|substructure',
            "horn√≠ stavba": r'horn√≠ stavba|superstructure'
        }
    
    async def analyze_concrete(self, doc_paths: List[str], smeta_path: str) -> Dict[str, Any]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å –≤—ã–±–æ—Ä–æ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {len(doc_paths)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # –í—Å–µ–≥–¥–∞ –Ω–∞—á–∏–Ω–∞–µ–º —Å –±—ã—Å—Ç—Ä–æ–≥–æ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        start_time = asyncio.get_event_loop().time()
        local_analysis = await self._fast_local_analysis(doc_paths, smeta_path)
        local_time = asyncio.get_event_loop().time() - start_time
        
        logger.info(f"‚ö° –õ–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {local_time:.2f}—Å")
        
        # –†–µ—à–∞–µ–º, –Ω—É–∂–µ–Ω –ª–∏ Claude
        if self._should_use_claude(local_analysis):
            try:
                logger.info("ü§ñ –ó–∞–ø—É—Å–∫ Claude enhancement")
                claude_start = asyncio.get_event_loop().time()
                
                all_text = await self._extract_all_text(doc_paths)
                smeta_data = self._parse_smeta(smeta_path)
                
                enhanced_analysis = await self.claude_client.enhance_analysis(
                    local_analysis, all_text, smeta_data
                )
                
                claude_time = asyncio.get_event_loop().time() - claude_start
                logger.info(f"ü§ñ Claude –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {claude_time:.2f}—Å")
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                return self._merge_analyses(local_analysis, enhanced_analysis, {
                    "local_time": local_time,
                    "claude_time": claude_time,
                    "total_time": local_time + claude_time
                })
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ Claude: {e}")
                local_analysis["claude_error"] = str(e)
                local_analysis["analysis_method"] = "local_fallback"
                
        local_analysis["analysis_method"] = "local_only"
        local_analysis["timing"] = {"total_time": local_time}
        return local_analysis
    
    async def _fast_local_analysis(self, doc_paths: List[str], smeta_path: str) -> Dict[str, Any]:
        """
        –ë—ã—Å—Ç—Ä—ã–π –ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
        """
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
        all_text = ""
        for doc_path in doc_paths:
            try:
                content = self.doc_parser.parse(doc_path)
                all_text += f"\n\n=== {os.path.basename(doc_path)} ===\n{content}"
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {doc_path}: {e}")
        
        all_text_lower = all_text.lower()
        
        # –ü–∞—Ä—Å–∏–º —Å–º–µ—Ç—É
        smeta_data = self._parse_smeta(smeta_path)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –±–µ—Ç–æ–Ω–Ω—ã–µ –º–∞—Ä–∫–∏
        concrete_data = {}
        
        # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        for pattern in self.concrete_patterns:
            for match in re.finditer(pattern, all_text_lower, re.IGNORECASE):
                grade = match.group(1) if match.groups() else match.group(0)
                grade = grade.upper().strip()
                
                if not grade or len(grade) < 2:
                    continue
                
                logger.debug(f"üîç –ù–∞–π–¥–µ–Ω–∞ –º–∞—Ä–∫–∞: {grade}")
                
                if grade not in concrete_data:
                    concrete_data[grade] = {
                        "environment_classes": set(),
                        "workability_classes": set(),
                        "used_in": set(),
                        "context_snippets": [],
                        "smeta_mentions": []
                    }
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç (200 —Å–∏–º–≤–æ–ª–æ–≤ –≤–æ–∫—Ä—É–≥)
                start = max(0, match.start() - 100)
                end = min(len(all_text_lower), match.end() + 100)
                snippet = all_text_lower[start:end]
                
                # –ò—â–µ–º –∫–ª–∞—Å—Å—ã —Å—Ä–µ–¥—ã
                env_classes = set(re.findall(self.env_classes_regex, snippet, re.IGNORECASE))
                concrete_data[grade]["environment_classes"].update(env_classes)
                
                # –ò—â–µ–º –∫–ª–∞—Å—Å—ã –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ü–∏–∏
                workability = set(re.findall(self.workability_regex, snippet, re.IGNORECASE))
                concrete_data[grade]["workability_classes"].update(workability)
                
                # –ò—â–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
                for context_name, context_pattern in self.context_patterns.items():
                    if re.search(context_pattern, snippet, re.IGNORECASE):
                        concrete_data[grade]["used_in"].add(context_name)
                        concrete_data[grade]["context_snippets"].append({
                            "element": context_name,
                            "snippet": snippet.strip()[:150],
                            "confidence": "high"
                        })
        
        # –ü–æ–∏—Å–∫ –≤ —Å–º–µ—Ç–µ
        self._find_concrete_in_smeta(concrete_data, smeta_data)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = []
        for grade, data in concrete_data.items():
            result.append({
                "grade": grade,
                "used_in": sorted(list(data["used_in"])),
                "environment_classes": [
                    {
                        "code": cls.upper(),
                        "description": self.env_class_descriptions.get(cls.upper(), "Popis nen√≠ k dispozici.")
                    }
                    for cls in sorted(data["environment_classes"])
                ],
                "workability_classes": sorted(list(data["workability_classes"])),
                "context_snippets": data["context_snippets"][:5],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º
                "smeta_mentions": data["smeta_mentions"],
                "mentioned_in_docs": True
            })
        
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(result)} –º–∞—Ä–æ–∫ –±–µ—Ç–æ–Ω–∞")
        
        return {
            "concrete_summary": result,
            "analysis_stats": {
                "documents_processed": len(doc_paths),
                "concrete_grades_found": len(result),
                "smeta_rows_analyzed": len(smeta_data) if smeta_data else 0,
                "total_text_length": len(all_text)
            }
        }
    
    def _find_concrete_in_smeta(self, concrete_data: Dict, smeta_data: List[Dict]):
        """–ü–æ–∏—Å–∫ –º–∞—Ä–æ–∫ –≤ —Å–º–µ—Ç–µ (—É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        if not smeta_data:
            return
        
        for row_idx, row in enumerate(smeta_data):
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è
            row_text = " ".join([
                str(row.get('description', '')),
                str(row.get('material', '')),
                str(row.get('specification', '')),
                str(row.get('name', ''))
            ]).lower()
            
            for grade in concrete_data.keys():
                if grade.lower() in row_text:
                    concrete_data[grade]["smeta_mentions"].append({
                        "row": row_idx + 1,
                        "description": row_text.strip()[:200],
                        "code": row.get("code", ""),
                        "qty": row.get("qty", 0),
                        "unit": row.get("unit", "")
                    })
    
    def _should_use_claude(self, local_analysis: Dict) -> bool:
        """–†–µ—à–µ–Ω–∏–µ –æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ Claude –∞–Ω–∞–ª–∏–∑–∞"""
        if not self.claude_client or not self.use_claude:
            return False
        
        stats = local_analysis.get("analysis_stats", {})
        concrete_count = stats.get("concrete_grades_found", 0)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Claude –µ—Å–ª–∏:
        # 1. –ù–∞–π–¥–µ–Ω–æ –º–∞–ª–æ –º–∞—Ä–æ–∫ (–º–æ–∂–µ—Ç –ø—Ä–æ–ø—É—Å—Ç–∏–ª–∏)
        # 2. –°–ª–æ–∂–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        # 3. –†–µ–∂–∏–º "enhancement" –∏–ª–∏ "primary"
        
        if self.claude_mode == "primary":
            return True
        elif self.claude_mode == "enhancement":
            return concrete_count < 3 or stats.get("total_text_length", 0) > 10000
        else:  # fallback
            return concrete_count == 0
    
    def _merge_analyses(self, local: Dict, enhanced: Dict, timing: Dict) -> Dict:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∏ Claude –∞–Ω–∞–ª–∏–∑–æ–≤"""
        # –ë–µ—Ä–µ–º –ª—É—á—à–µ–µ –∏–∑ –æ–±–æ–∏—Ö
        result = {
            "concrete_summary": enhanced.get("claude_concrete_analysis", {}).get("claude_analysis", local["concrete_summary"]),
            "local_analysis": local,
            "claude_analysis": enhanced.get("claude_concrete_analysis", {}),
            "materials_analysis": enhanced.get("claude_materials_analysis", {}),
            "analysis_method": "hybrid_local_claude",
            "enhanced": True,
            "timing": timing
        }
        
        return result
    
    async def _extract_all_text(self, doc_paths: List[str]) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è Claude"""
        all_text = ""
        for doc_path in doc_paths:
            try:
                content = self.doc_parser.parse(doc_path)
                all_text += f"\n\n=== {os.path.basename(doc_path)} ===\n{content}"
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ {doc_path}: {e}")
        return all_text[:8000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è Claude
    
    def _parse_smeta(self, smeta_path: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å–º–µ—Ç—ã"""
        try:
            if smeta_path.endswith('.xml'):
                return self.xml_parser.parse(smeta_path)
            elif smeta_path.endswith(('.xls', '.xlsx')):
                return self.excel_parser.parse(smeta_path)
            else:
                return []
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–º–µ—Ç—ã: {e}")
            return []

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è API
async def analyze_concrete(
    doc_paths: List[str], 
    smeta_path: str, 
    use_claude: bool = True,
    claude_mode: str = "enhancement"
) -> Dict[str, Any]:
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–µ—Ç–æ–Ω–Ω—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
    
    Args:
        doc_paths: –ü—É—Ç–∏ –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
        smeta_path: –ü—É—Ç—å –∫ —Å–º–µ—Ç–µ  
        use_claude: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Claude API
        claude_mode: "enhancement" | "primary" | "fallback"
    """
    agent = HybridConcreteAnalysisAgent(use_claude=use_claude, claude_mode=claude_mode)
    return await agent.analyze_concrete(doc_paths, smeta_path)

def analyze_concrete_sync(
    doc_paths: List[str], 
    smeta_path: str, 
    use_claude: bool = True,
    claude_mode: str = "enhancement"
) -> Dict[str, Any]:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è"""
    return asyncio.run(analyze_concrete(doc_paths, smeta_path, use_claude, claude_mode))
