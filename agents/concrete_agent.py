"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π ConcreteAgentHybrid ‚Äî –∞–≥–µ–Ω—Ç –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –±–µ—Ç–æ–Ω–æ–≤.
agents/concrete_agent.py - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
"""

import re
import os
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass

try:
    from services.doc_parser import DocParser
except ImportError:
    # Fallback to old location with warning
    import warnings
    warnings.warn("parsers.doc_parser is deprecated. Use services.doc_parser instead.", DeprecationWarning)
    from parsers.doc_parser import DocParser
from parsers.smeta_parser import SmetaParser
from services.doc_parser import parse_document  # New unified parser

# Import centralized services  
try:
    from app.core.llm_service import get_llm_service
    from app.core.prompt_loader import get_prompt_loader
    CENTRALIZED_LLM_AVAILABLE = True
except ImportError:
    CENTRALIZED_LLM_AVAILABLE = False
    # Fallback to old client
    from utils.claude_client import get_claude_client

from config.settings import settings
from outputs.save_report import save_merged_report
from utils.czech_preprocessor import get_czech_preprocessor
from utils.volume_analyzer import get_volume_analyzer
from utils.report_generator import get_report_generator
from utils.knowledge_base_service import get_knowledge_service
from agents.concrete_volume_agent import get_concrete_volume_agent

logger = logging.getLogger(__name__)

@dataclass
class ConcreteMatch:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω–æ–π –º–∞—Ä–∫–∏ –±–µ—Ç–æ–Ω–∞"""
    grade: str
    context: str
    location: str
    confidence: float
    method: str
    coordinates: Optional[Tuple[int, int, int, int]] = None

@dataclass 
class StructuralElement:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞"""
    name: str
    concrete_grade: Optional[str]
    location: str
    context: str

class ConcreteAgentHybrid:
    def __init__(self, knowledge_base_path="knowledge_base/complete-concrete-knowledge-base.json"):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ —Å–µ—Ä–≤–∏—Å—ã
        self.knowledge_service = get_knowledge_service()
        self.volume_agent = get_concrete_volume_agent()
        self.volume_analyzer = get_volume_analyzer()  # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.report_generator = get_report_generator()
        self.czech_preprocessor = get_czech_preprocessor()
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–ø—É—Å—Ç–∏–º—ã–µ –º–∞—Ä–∫–∏ –∏–∑ –Ω–æ–≤–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞
        self.allowed_grades = set(self.knowledge_service.get_all_concrete_grades())
        logger.info(f"üéØ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.allowed_grades)} –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –º–∞—Ä–æ–∫ –±–µ—Ç–æ–Ω–∞")
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        self.context_keywords = list(self.knowledge_service.get_context_keywords())
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        try:
            with open(knowledge_base_path, "r", encoding="utf-8") as f:
                self.knowledge_base = json.load(f)
            logger.info("üìö Legacy knowledge-base –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å legacy knowledge-base: {e}")
            self.knowledge_base = self._create_default_kb()

        self.doc_parser = DocParser()
        self.smeta_parser = SmetaParser()
        
        # Initialize LLM services
        if CENTRALIZED_LLM_AVAILABLE:
            self.llm_service = get_llm_service()
            self.prompt_loader = get_prompt_loader()
            self.use_centralized_llm = True
            logger.info("‚úÖ Using centralized LLM service")
        else:
            self.claude_client = get_claude_client()  # Fallback to old client
            self.use_centralized_llm = False
            logger.warning("‚ö†Ô∏è Using legacy Claude client - update recommended")

        # –°—Ç—Ä–æ–≥–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –º–∞—Ä–æ–∫ –±–µ—Ç–æ–Ω–∞ (—Ç–æ–ª—å–∫–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã)
        self.concrete_pattern = r'\b(?:LC|C)\d{1,3}/\d{1,3}\b'

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —á–µ—à—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        self.context_keywords = [
            # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã —Å –¥–∏–∞–∫—Ä–∏—Ç–∏–∫–æ–π
            'beton', 'betonu', 'betonem', 'betony', 'betonov√©', 'betonov√Ωch', 'beton√°≈ôsk',
            't≈ô√≠da', 't≈ô√≠dy', 't≈ô√≠du', 't≈ô√≠dƒõ', 't≈ô√≠d',
            'stupe≈à', 'stupnƒõ', 'stupni',
            'odolnost', 'odolnosti',
            'pevnost', 'pevnosti',
            
            # –ö–æ–¥—ã –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è
            'XC1', 'XC2', 'XC3', 'XC4',
            'XA1', 'XA2', 'XA3', 
            'XF1', 'XF2', 'XF3', 'XF4',
            'XD1', 'XD2', 'XD3',
            'XM1', 'XM2', 'XM3', 'X0',
            
            # –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (—Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –¥–∏–∞–∫—Ä–∏—Ç–∏–∫–æ–π)
            'd≈ô√≠k', 'z√°kladov√°', 'ƒç√°st', 'piloty', 'vrtan√©',
            '≈ô√≠ms', 'opƒõrn√°', 'zeƒè', 'konstrukce', 'nosn√°',
            'podkladn√≠', 'vrstva', 'v√Ωztu≈æ', 'ocel',
            'izolace', 'vodotƒõsn', 'dren√°≈æn', '≈°tƒõrkodrt',
            '≈æelezobetonov√°', 'monolitick√Ω', 'prefabrik√°t'
        ]

        # –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (—á–µ—à—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã)
        self.structural_elements = {
            'OPƒöRA': {'en': 'abutment', 'applications': ['XC4', 'XD1', 'XF2']},
            'PIL√ç≈ò': {'en': 'pier', 'applications': ['XC4', 'XD3', 'XF4']},
            '≈ò√çMSA': {'en': 'cornice', 'applications': ['XC4', 'XD3', 'XF4']},
            'MOSTOVKA': {'en': 'deck', 'applications': ['XC4', 'XD3', 'XF4']},
            'Z√ÅKLAD': {'en': 'foundation', 'applications': ['XC1', 'XC2', 'XA1']},
            'VOZOVKA': {'en': 'roadway', 'applications': ['XF2', 'XF4', 'XD1']},
            'GAR√Å≈Ω': {'en': 'garage', 'applications': ['XD1']},
            'STƒöNA': {'en': 'wall', 'applications': ['XC1', 'XC3', 'XC4']},
            'SLOUP': {'en': 'column', 'applications': ['XC1', 'XC3']},
            'SCHODI≈†Tƒö': {'en': 'stairs', 'applications': ['XC1', 'XC3']},
            'PODKLADN√ç': {'en': 'subgrade', 'applications': ['X0', 'XC1']},
            'NOSN√Å': {'en': 'load-bearing', 'applications': ['XC1', 'XC3']},
            'VƒöNEC': {'en': 'tie-beam', 'applications': ['XC2', 'XF1']},
            'DESKA': {'en': 'slab', 'applications': ['XC1', 'XC2']},
            'PREFABRIK√ÅT': {'en': 'precast', 'applications': ['XC1', 'XC3']},
            'MONOLITICK√ù': {'en': 'monolithic', 'applications': ['XC1', 'XC2']},
        }

        # –ö–æ–¥—ã —Å–º–µ—Ç –∏ –∏—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
        self.smeta_codes = {
            '801': 'Prost√Ω beton',
            '802': '≈Ωelezobeton monolitick√Ω',
            '803': '≈Ωelezobeton prefabrikovan√Ω',
            '811': 'Betony speci√°ln√≠',
            '271': 'Konstrukce betonov√© a ≈æelezobetonov√©',
            'HSV': 'Hlavn√≠ stavebn√≠ v√Ωroba',
            'PSV': 'P≈ôidru≈æen√° stavebn√≠ v√Ωroba',
        }

        # –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —á–µ—à—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        self.czech_preprocessor = get_czech_preprocessor()

    def _extract_valid_grades_from_kb(self, kb: Dict) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–ø—É—Å—Ç–∏–º—ã–µ –º–∞—Ä–∫–∏ –±–µ—Ç–æ–Ω–∞ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        grades = []
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫–ª–∞—Å—Å—ã –ø—Ä–æ—á–Ω–æ—Å—Ç–∏
        standard_classes = kb.get("strength_classes", {}).get("standard", {})
        if standard_classes:
            grades.extend(standard_classes.keys())
        
        # UHPC –∫–ª–∞—Å—Å—ã
        uhpc_classes = kb.get("strength_classes", {}).get("uhpc", {})
        if uhpc_classes:
            grades.extend(uhpc_classes.keys())
        
        # –õ–µ–≥–∫–∏–π –±–µ—Ç–æ–Ω
        lightweight_classes = kb.get("concrete_types_by_density", {}).get("lehk√Ω_beton", {}).get("strength_classes", [])
        if lightweight_classes:
            grades.extend(lightweight_classes)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Å–µ –º–∞—Ä–∫–∏
        normalized_grades = []
        for grade in grades:
            normalized = self._normalize_concrete_grade(grade)
            if normalized and self._is_valid_grade_format(normalized):
                normalized_grades.append(normalized)
        
        return normalized_grades

    def _create_default_kb(self) -> Dict:
        """–°–æ–∑–¥–∞–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        return {
            "strength_classes": {
                "standard": {
                    "C12/15": {"fck": 12, "fcm": 20},
                    "C16/20": {"fck": 16, "fcm": 24},
                    "C20/25": {"fck": 20, "fcm": 28},
                    "C25/30": {"fck": 25, "fcm": 33},
                    "C30/37": {"fck": 30, "fcm": 38},
                    "C35/45": {"fck": 35, "fcm": 43},
                    "C40/50": {"fck": 40, "fcm": 48},
                    "C45/55": {"fck": 45, "fcm": 53},
                    "C50/60": {"fck": 50, "fcm": 58}
                },
                "uhpc": {
                    "C80/95": {"fck": 80, "fcm": 88},
                    "C90/105": {"fck": 90, "fcm": 98}
                }
            },
            "concrete_types_by_density": {
                "lehk√Ω_beton": {
                    "strength_classes": ["LC12/13", "LC16/18", "LC20/22", "LC25/28", "LC30/33", "LC35/38", "LC40/44", "LC45/50", "LC50/55", "LC55/60", "LC60/66", "LC70/77", "LC80/88"]
                }
            },
            "environment_classes": {
                "XC1": {"description": "such√© nebo st√°le mokr√©", "applications": ["interi√©r", "z√°klady"]},
                "XC2": {"description": "mokr√©, obƒças such√©", "applications": ["z√°klady", "spodn√≠ stavba"]},
                "XC3": {"description": "st≈ôednƒõ mokr√©", "applications": ["kryt√© prostory"]},
                "XC4": {"description": "st≈ô√≠davƒõ mokr√© a such√©", "applications": ["vnƒõj≈°√≠ povrchy"]},
                "XD1": {"description": "chloridy - m√≠rn√©", "applications": ["gar√°≈æe", "parkovi≈°tƒõ"]},
                "XD3": {"description": "chloridy - siln√©", "applications": ["mosty", "vozovky"]},
                "XF1": {"description": "mr√°z - m√≠rn√Ω", "applications": ["vnƒõj≈°√≠ svisl√© plochy"]},
                "XF2": {"description": "mr√°z + soli", "applications": ["silniƒçn√≠ konstrukce"]},
                "XF4": {"description": "mr√°z + soli - extr√©mn√≠", "applications": ["mostovky", "vozovky"]},
                "XA1": {"description": "chemicky agresivn√≠ - slabƒõ", "applications": ["z√°klady", "septiky"]},
            }
        }

    def _is_valid_grade_format(self, grade: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º–∞—Ç–∞ –º–∞—Ä–∫–∏ –±–µ—Ç–æ–Ω–∞"""
        if not grade or len(grade) > 8:
            return False
        
        if '/' not in grade:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ —á–∏—Å–ª–∞
        if re.match(r'C\d{4,}', grade):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        if not re.match(r'^(?:LC|C)\d{1,3}/\d{1,3}$', grade):
            return False
        
        return True

    def _has_concrete_context(self, text: str, start_pos: int, end_pos: int) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤ —Ä—è–¥–æ–º —Å –º–∞—Ä–∫–æ–π –±–µ—Ç–æ–Ω–∞"""
        context_window = 50
        context_start = max(0, start_pos - context_window)
        context_end = min(len(text), end_pos + context_window)
        context = text[context_start:context_end].lower()
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤
        keywords_pattern = '|'.join(re.escape(keyword.lower()) for keyword in self.context_keywords)
        
        return bool(re.search(rf'\b({keywords_pattern})\b', context, re.IGNORECASE))

    def _local_concrete_analysis(self, text: str) -> Dict[str, Any]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–æ–π —á–µ—à—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        
        # –ù–û–í–û–ï: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—à—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        preprocessed = self.czech_preprocessor.preprocess_document_text(text)
        fixed_text = preprocessed['fixed_text']
        
        logger.info(f"üá®üáø –ü—Ä–∏–º–µ–Ω–µ–Ω–æ {preprocessed['changes_count']} –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–∏–∞–∫—Ä–∏—Ç–∏–∫–∏")
        
        all_matches = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞
        for match in re.finditer(self.concrete_pattern, fixed_text, re.IGNORECASE):
            grade = self._normalize_concrete_grade(match.group().strip())
            
            # –§–∏–ª—å—Ç—Ä 1: –ú–∞—Ä–∫–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ whitelist –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
            if grade not in self.allowed_grades:
                continue
            
            start_pos, end_pos = match.start(), match.end()
            
            # –£–õ–£–ß–®–ï–ù–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –±–û–ª—å—à–∏–º –æ–∫–Ω–æ–º (150 —Å–∏–º–≤–æ–ª–æ–≤)
            context = self.czech_preprocessor.enhance_context_window(
                fixed_text, start_pos, end_pos, window_size=150
            )
            
            # –§–∏–ª—å—Ç—Ä 2: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç (–Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤)
            if not self._has_concrete_context(context, 0, len(context)):
                continue
            
            # –§–∏–ª—å—Ç—Ä 3: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞
            if not self._is_valid_grade_format(grade):
                continue
            
            # –£–õ–£–ß–®–ï–ù–ù–û–ï –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
            location = self.czech_preprocessor.identify_construction_element_enhanced(context)
            
            all_matches.append(ConcreteMatch(
                grade=grade,
                context=context.strip(),
                location=location,
                confidence=0.95,
                method='regex_enhanced_czech',
                coordinates=None
            ))
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ª—É—á—à–∏—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        unique_matches = {}
        for match in all_matches:
            key = match.grade
            if key not in unique_matches or match.confidence > unique_matches[key].confidence:
                unique_matches[key] = match
        
        return {
            'concrete_summary': [
                {
                    'grade': match.grade,
                    'location': match.location,
                    'context': match.context[:200],
                    'confidence': match.confidence,
                    'method': match.method
                }
                for match in unique_matches.values()
            ],
            'analysis_method': 'local_enhanced_czech',
            'total_matches': len(unique_matches),
            'success': True,
            'allowed_grades_count': len(self.allowed_grades),
            'czech_preprocessing': {
                'diacritic_fixes': preprocessed['changes_count'],
                'original_length': preprocessed['original_length'],
                'fixed_length': preprocessed['fixed_length']
            }
        }

    def _normalize_concrete_grade(self, grade: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ –º–∞—Ä–∫–∏ –±–µ—Ç–æ–Ω–∞"""
        if not grade:
            return ""
        
        # –ë–∞–∑–æ–≤–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        normalized = grade.upper().strip().replace(" ", "")
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        normalized = re.sub(r'[^\w/]', '', normalized)
        
        return normalized

    def _identify_structural_element(self, context: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —á–µ—à—Å–∫–æ–≥–æ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞"""
        return self.czech_preprocessor.identify_construction_element_enhanced(context)

    def _analyze_volumes_from_documents(self, doc_paths: List[str], smeta_path: Optional[str] = None) -> List:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—ä–µ–º—ã –±–µ—Ç–æ–Ω–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Å–º–µ—Ç"""
        all_volumes = []
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        for doc_path in doc_paths:
            try:
                # Use unified parser with MinerU integration
                result = parse_document(doc_path)
                if result["success"] and result["results"]:
                    # Extract text content from parse results
                    text = ""
                    for parse_result in result["results"]:
                        content = parse_result.content
                        if isinstance(content, str):
                            text += content + "\n"
                        elif isinstance(content, dict):
                            # Handle structured content from MinerU
                            text += content.get("text", str(content)) + "\n"
                        else:
                            text += str(content) + "\n"
                    
                    if text.strip():
                        volumes = self.volume_analyzer.analyze_volumes_from_text(
                            text, Path(doc_path).name
                        )
                        all_volumes.extend(volumes)
                        logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ –æ–±—ä–µ–º–æ–≤ –≤ {Path(doc_path).name}: {len(volumes)} (parser: {result['results'][0].parser_used if result['results'] else 'unknown'})")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –æ–±—ä–µ–º–æ–≤ –≤ {doc_path}: {e}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–º–µ—Ç—É, –µ—Å–ª–∏ –µ—Å—Ç—å
        if smeta_path and os.path.exists(smeta_path):
            try:
                # Use unified parser for smeta as well
                result = parse_document(smeta_path)
                if result["success"] and result["results"]:
                    smeta_text = ""
                    for parse_result in result["results"]:
                        content = parse_result.content
                        if isinstance(content, str):
                            smeta_text += content + "\n"
                        elif isinstance(content, (list, dict)):
                            # Handle structured smeta data
                            smeta_text += str(content) + "\n"
                        else:
                            smeta_text += str(content) + "\n"
                    
                    if smeta_text.strip():
                        smeta_volumes = self.volume_analyzer.analyze_volumes_from_text(
                            smeta_text, Path(smeta_path).name
                        )
                        all_volumes.extend(smeta_volumes)
                    logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ –æ–±—ä–µ–º–æ–≤ –≤ —Å–º–µ—Ç–µ: {len(smeta_volumes)}")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–º–µ—Ç—ã {smeta_path}: {e}")
        
        return all_volumes

    async def analyze_with_volumes(self, doc_paths: List[str], smeta_path: Optional[str] = None,
                                   use_claude: bool = True, claude_mode: str = "enhancement",
                                   language: str = "cz") -> Dict[str, Any]:
        """
        –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –æ–±—ä–µ–º–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –æ—Ç—á–µ—Ç–æ–≤
        """
        logger.info(f"üèóÔ∏è –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å –æ–±—ä–µ–º–∞–º–∏ (—è–∑—ã–∫: {language})")
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑ –º–∞—Ä–æ–∫ –±–µ—Ç–æ–Ω–∞
        concrete_analysis = await self.analyze(doc_paths, smeta_path, use_claude, claude_mode)
        
        # –ê–Ω–∞–ª–∏–∑ –æ–±—ä–µ–º–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–æ–≤–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
        volumes = await self.volume_agent.analyze_volumes_from_documents(doc_paths, smeta_path)
        logger.info(f"üìä –ù–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –æ–±—ä–µ–º–æ–≤: –Ω–∞–π–¥–µ–Ω–æ {len(volumes)} –ø–æ–∑–∏—Ü–∏–π")
        
        # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–∫—É –ø–æ –æ–±—ä–µ–º–∞–º
        volume_summary = self.volume_agent.create_volume_summary(volumes)
        
        # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
        enhanced_result = self._merge_concrete_and_volumes(concrete_analysis, volumes, volume_summary)
        logger.info("üîó –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π —á–µ—Ä–µ–∑ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —è–∑—ã–∫–∞ –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤
        self.report_generator = get_report_generator(language)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        enhanced_result.update({
            'volume_entries_found': len(volumes),
            'volume_summary': volume_summary,
            'report_language': language,
            'analysis_type': 'complete_with_volumes_v2'
        })
        
        return enhanced_result

    def _merge_concrete_and_volumes(self, concrete_analysis: Dict[str, Any], 
                                   volumes: List, volume_summary: Dict[str, Any]) -> Dict[str, Any]:
        """
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –º–∞—Ä–æ–∫ –∏ –æ–±—ä–µ–º–æ–≤ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
        """
        enhanced_result = concrete_analysis.copy()
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤–æ–¥–∫—É –ø–æ –æ–±—ä–µ–º–∞–º
        enhanced_result['volume_summary'] = volume_summary
        
        # –£–ª—É—á—à–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–∂–¥–æ–π –Ω–∞–π–¥–µ–Ω–Ω–æ–π –º–∞—Ä–∫–µ
        enhanced_summary = []
        for concrete_item in concrete_analysis.get('concrete_summary', []):
            grade = concrete_item['grade']
            enhanced_item = concrete_item.copy()
            
            # –ò—â–µ–º –æ–±—ä–µ–º—ã –¥–ª—è —ç—Ç–æ–π –º–∞—Ä–∫–∏
            grade_volumes = [v for v in volumes if hasattr(v, 'concrete_grade') and v.concrete_grade == grade]
            
            if grade_volumes:
                total_volume = sum(v.volume_m3 for v in grade_volumes)
                total_cost = sum(v.total_cost or 0 for v in grade_volumes)
                
                enhanced_item.update({
                    'total_volume_m3': round(total_volume, 2),
                    'total_cost': round(total_cost, 2) if total_cost > 0 else None,
                    'volume_entries_count': len(grade_volumes),
                    'construction_elements': list(set(v.construction_element for v in grade_volumes))
                })
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è —á–µ—Ä–µ–∑ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
                for element in enhanced_item['construction_elements']:
                    validation = self.knowledge_service.validate_analysis_result(grade, element)
                    if not validation['valid'] or validation['warnings']:
                        if 'validation_warnings' not in enhanced_item:
                            enhanced_item['validation_warnings'] = []
                        enhanced_item['validation_warnings'].extend(validation['warnings'])
                        
                        if validation['recommendations']:
                            if 'recommendations' not in enhanced_item:
                                enhanced_item['recommendations'] = []
                            enhanced_item['recommendations'].extend(validation['recommendations'])
            
            enhanced_summary.append(enhanced_item)
        
        enhanced_result['concrete_summary'] = enhanced_summary
        return enhanced_result

    def save_comprehensive_reports(self, analysis_result: Dict[str, Any], 
                                   output_dir: str = "outputs", 
                                   language: str = "cz") -> Dict[str, str]:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ –≤–∏–¥—ã –æ—Ç—á–µ—Ç–æ–≤
        """
        saved_files = {}
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç—á–µ—Ç–æ–≤
        generator = get_report_generator(language)
        
        # Markdown –æ—Ç—á–µ—Ç
        markdown_file = output_path / f"concrete_analysis_report_{language}.md"
        if generator.save_markdown_report(analysis_result, str(markdown_file)):
            saved_files['markdown'] = str(markdown_file)
        
        # JSON –¥–∞–Ω–Ω—ã–µ –¥–ª—è Excel
        json_file = output_path / f"concrete_analysis_data_{language}.json"
        if generator.save_json_data(analysis_result, str(json_file)):
            saved_files['json'] = str(json_file)
        
        # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π JSON –æ—Ç—á–µ—Ç
        json_report_file = output_path / "concrete_analysis_report.json"
        try:
            save_merged_report(analysis_result, str(json_report_file))
            saved_files['json_report'] = str(json_report_file)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å JSON –æ—Ç—á–µ—Ç: {e}")
        
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(saved_files)} —Ñ–∞–π–ª–æ–≤ –æ—Ç—á–µ—Ç–æ–≤")
        return saved_files
    
    def _is_llm_available(self) -> bool:
        """Check if LLM service is available (centralized or legacy)"""
        if self.use_centralized_llm:
            return self.llm_service is not None
        else:
            return self.claude_client and self.claude_client.is_available

    async def _claude_concrete_analysis(self, text: str, smeta_data: List[Dict]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Claude —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        if self.use_centralized_llm:
            return await self._centralized_llm_analysis(text, smeta_data)
        else:
            # Legacy fallback
            if not self.claude_client or not self.claude_client.is_available:
                logger.info("Claude API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ —Å Claude")
                return {
                    "success": False, 
                    "error": "Claude API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ ANTHROPIC_API_KEY",
                    "error_type": "api_unavailable"
                }
            
            try:
                result = await self.claude_client.analyze_concrete_with_claude(text, smeta_data)
                
                return {
                    'concrete_summary': result.get('claude_analysis', {}).get('concrete_grades', []),
                    'analysis_method': 'claude_enhanced',
                    'success': True,
                    'tokens_used': result.get('tokens_used', 0),
                    'raw_response': result.get('raw_response', '')
                }
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ Claude –∞–Ω–∞–ª–∏–∑–∞: {e}")
                return {"success": False, "error": str(e)}
    
    async def _centralized_llm_analysis(self, text: str, smeta_data: List[Dict]) -> Dict[str, Any]:
        """Analysis using centralized LLM service"""
        try:
            # Get system prompt from centralized prompt loader
            system_prompt = self.prompt_loader.get_system_prompt("concrete")
            prompt_config = self.prompt_loader.get_prompt_config("concrete")
            
            provider = prompt_config.get("provider", "claude")
            model = prompt_config.get("model")
            
            # Prepare context with smeta data
            smeta_context = ""
            if smeta_data:
                smeta_context = "\n\n=== –°–ú–ï–¢–ê –î–ê–ù–ù–´–ï ===\n"
                for item in smeta_data[:10]:  # Limit to avoid token overflow
                    if isinstance(item, dict):
                        desc = item.get('description', '')
                        qty = item.get('quantity', item.get('qty', ''))
                        unit = item.get('unit', '')
                        smeta_context += f"- {desc} ({qty} {unit})\n"
            
            user_prompt = f"""
            –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –º–∞—Ä–æ–∫ –±–µ—Ç–æ–Ω–∞ –∏ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤.
            
            –î–æ–∫—É–º–µ–Ω—Ç:
            {text[:10000]}  # Limit text length
            
            {smeta_context}
            
            –ù–∞–π–¥–∏ –≤—Å–µ –º–∞—Ä–∫–∏ –±–µ—Ç–æ–Ω–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ C25/30, C30/37 –∏ —Ç.–¥., —Å —É–∫–∞–∑–∞–Ω–∏–µ–º:
            - –ö–ª–∞—Å—Å—ã –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è (XC1, XF1, –∏ —Ç.–¥.)
            - –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (—Å—Ç–µ–Ω—ã, —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç, –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è)
            - –û–±—ä–µ–º—ã (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã)
            
            –í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ.
            """
            
            response = await self.llm_service.run_prompt(
                provider=provider,
                prompt=user_prompt,
                system_prompt=system_prompt,
                model=model
            )
            
            if response.get("success"):
                content = response.get("content", "")
                
                # Try to parse JSON from response
                try:
                    import json
                    if content.strip().startswith('{'):
                        analysis_data = json.loads(content)
                    else:
                        # Extract JSON if wrapped in text
                        import re
                        json_match = re.search(r'\{.*\}', content, re.DOTALL)
                        if json_match:
                            analysis_data = json.loads(json_match.group())
                        else:
                            analysis_data = {"raw_text": content}
                    
                    return {
                        'concrete_summary': analysis_data.get('concrete_grades', []),
                        'analysis_method': 'centralized_llm_enhanced',
                        'success': True,
                        'tokens_used': response.get('usage', {}).get('total_tokens', 0),
                        'raw_response': content,
                        'provider': provider,
                        'model': model
                    }
                    
                except json.JSONDecodeError:
                    # Return raw response if JSON parsing fails
                    return {
                        'concrete_summary': [],
                        'analysis_method': 'centralized_llm_text',
                        'success': True,
                        'tokens_used': response.get('usage', {}).get('total_tokens', 0),
                        'raw_response': content,
                        'provider': provider,
                        'model': model
                    }
            else:
                logger.error(f"‚ùå LLM service failed: {response.get('error')}")
                return {"success": False, "error": response.get('error', 'Unknown error')}
                
        except Exception as e:
            logger.error(f"‚ùå Centralized LLM analysis error: {e}")
            return {"success": False, "error": str(e)}

    async def analyze(self, doc_paths: List[str], smeta_path: Optional[str] = None,
                      use_claude: bool = True, claude_mode: str = "enhancement") -> Dict[str, Any]:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
        """
        logger.info(f"üèóÔ∏è –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (—Ä–µ–∂–∏–º: {claude_mode})")
        
        all_text = ""
        processed_docs = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        for doc_path in doc_paths:
            try:
                # Use unified parser with MinerU integration
                result = parse_document(doc_path)
                if result["success"] and result["results"]:
                    text = ""
                    for parse_result in result["results"]:
                        content = parse_result.content
                        if isinstance(content, str):
                            text += content + "\n"
                        elif isinstance(content, dict):
                            # Handle structured content from MinerU
                            text += content.get("text", str(content)) + "\n"
                        else:
                            text += str(content) + "\n"
                    
                    all_text += text + "\n"
                    
                    processed_docs.append({
                        'file': Path(doc_path).name,
                        'type': 'Document',
                        'text_length': len(text),
                        'parser_used': result["results"][0].parser_used if result["results"] else "unknown"
                    })
                else:
                    processed_docs.append({
                        'file': Path(doc_path).name, 
                        'error': result.get("error", "Unknown parsing error")
                    })
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {doc_path}: {e}")
                processed_docs.append({'file': Path(doc_path).name, 'error': str(e)})
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–º–µ—Ç—É
        smeta_data = []
        if smeta_path:
            try:
                # Use unified parser for smeta
                result = parse_document(smeta_path)
                if result["success"] and result["results"]:
                    for parse_result in result["results"]:
                        content = parse_result.content
                        if isinstance(content, list):
                            # Structured smeta data from parser
                            smeta_data.extend(content)
                        elif isinstance(content, dict) and "items" in content:
                            # Legacy format with items key
                            smeta_data.extend(content["items"])
                        elif isinstance(content, dict):
                            # Single structured item
                            smeta_data.append(content)
                    
                    logger.info(f"üìä –°–º–µ—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞: {len(smeta_data)} –ø–æ–∑–∏—Ü–∏–π (parser: {result['results'][0].parser_used if result['results'] else 'unknown'})")
                else:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–º–µ—Ç—ã: {result.get('error', 'Unknown error')}")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–º–µ—Ç—ã: {e}")
        
        # –õ–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (—Ç–µ–ø–µ—Ä—å —É–ª—É—á—à–µ–Ω–Ω—ã–π —Å —á–µ—à—Å–∫–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–æ–π)
        local_result = self._local_concrete_analysis(all_text)
        
        # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Claude
        final_result = local_result.copy()
        
        if use_claude and self._is_llm_available():
            claude_result = await self._claude_concrete_analysis(all_text, smeta_data)
            
            if claude_mode == "primary" and claude_result.get("success"):
                final_result = claude_result
            elif claude_mode == "enhancement" and claude_result.get("success"):
                final_result.update({
                    'claude_analysis': claude_result,
                    'analysis_method': 'hybrid_enhanced_czech',
                    'total_tokens_used': claude_result.get('tokens_used', 0)
                })
        elif use_claude:
            # Claude was requested but is not available
            logger.warning("Claude –∞–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—à–µ–Ω, –Ω–æ LLM —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            final_result.update({
                'claude_unavailable': True,
                'claude_error': 'LLM —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ API –∫–ª—é—á–∏'
            })
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        final_result.update({
            'processed_documents': processed_docs,
            'smeta_items': len(smeta_data),
            'processing_time': 'completed',
            'knowledge_base_version': '3.0'
        })
        
        return final_result


# ==============================
# üîß –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ==============================

_hybrid_agent = None

def get_hybrid_agent() -> ConcreteAgentHybrid:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞"""
    global _hybrid_agent
    if _hybrid_agent is None:
        _hybrid_agent = ConcreteAgentHybrid()
        logger.info("ü§ñ ConcreteAgentHybrid –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    return _hybrid_agent

# ==============================
# üöÄ API-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ==============================

async def analyze_concrete(doc_paths: List[str], smeta_path: Optional[str] = None,
                           use_claude: bool = True, claude_mode: str = "enhancement") -> Dict[str, Any]:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–µ—Ç–æ–Ω–∞ - —Å–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º API
    """
    agent = get_hybrid_agent()
    
    try:
        result = await agent.analyze(doc_paths, smeta_path, use_claude, claude_mode)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        try:
            save_merged_report(result, "outputs/concrete_analysis_report.json")
            logger.info("üíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ outputs/concrete_analysis_report.json")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á—ë—Ç: {e}")
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ analyze_concrete: {e}")
        return {
            "error": str(e),
            "success": False,
            "analysis_method": "error",
            "concrete_summary": []
        }

async def analyze_concrete_with_volumes(doc_paths: List[str], smeta_path: Optional[str] = None,
                                        use_claude: bool = True, claude_mode: str = "enhancement",
                                        language: str = "cz") -> Dict[str, Any]:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –±–µ—Ç–æ–Ω–∞ —Å –æ–±—ä–µ–º–∞–º–∏ –∏ –æ—Ç—á–µ—Ç–∞–º–∏
    
    Args:
        doc_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
        smeta_path: –ü—É—Ç—å –∫ —Å–º–µ—Ç–µ –∏–ª–∏ v√Ωkaz v√Ωmƒõr (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        use_claude: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ Claude AI
        claude_mode: –†–µ–∂–∏–º Claude ("enhancement" –∏–ª–∏ "primary")
        language: –Ø–∑—ã–∫ –æ—Ç—á–µ—Ç–æ–≤ ("cz", "en", "ru")
        
    Returns:
        –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –æ–±—ä–µ–º–∞–º–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –æ—Ç—á–µ—Ç–æ–≤
    """
    agent = get_hybrid_agent()
    
    try:
        # –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –æ–±—ä–µ–º–∞–º–∏
        result = await agent.analyze_with_volumes(
            doc_paths, smeta_path, use_claude, claude_mode, language
        )
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤
        saved_files = agent.save_comprehensive_reports(result, "outputs", language)
        result['saved_reports'] = saved_files
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        concrete_count = len(result.get('concrete_summary', []))
        volume_count = result.get('volume_entries_found', 0)
        
        logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω:")
        logger.info(f"   - –ù–∞–π–¥–µ–Ω–æ –º–∞—Ä–æ–∫ –±–µ—Ç–æ–Ω–∞: {concrete_count}")
        logger.info(f"   - –ù–∞–π–¥–µ–Ω–æ –æ–±—ä–µ–º–æ–≤: {volume_count}")
        logger.info(f"   - –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –æ—Ç—á–µ—Ç–æ–≤: {len(saved_files)}")
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ analyze_concrete_with_volumes: {e}")
        return {
            "error": str(e),
            "success": False,
            "analysis_method": "error",
            "concrete_summary": []
        }
