"""
–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ConcreteAgent - –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–µ—Ç–æ–Ω–∞
agents/concrete_agent.py - –§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø
"""

import re
import os
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass

# –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –ø–∞—Ä—Å–µ—Ä–æ–≤
try:
    from services.doc_parser import DocParser
except ImportError:
    from parsers.doc_parser import DocParser

from parsers.smeta_parser import SmetaParser

# –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã
try:
    from app.core.llm_service import get_llm_service
    from app.core.prompt_loader import get_prompt_loader
    CENTRALIZED_LLM_AVAILABLE = True
except ImportError:
    CENTRALIZED_LLM_AVAILABLE = False
    try:
        from utils.claude_client import get_claude_client
    except ImportError:
        get_claude_client = None

# –£—Ç–∏–ª–∏—Ç—ã
from utils.czech_preprocessor import get_czech_preprocessor
from utils.volume_analyzer import get_volume_analyzer
from utils.report_generator import get_report_generator
from utils.knowledge_base_service import get_knowledge_service

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã
try:
    from agents.concrete_volume_agent import get_concrete_volume_agent
except ImportError:
    get_concrete_volume_agent = None

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤
try:
    from outputs.save_report import save_merged_report
except ImportError:
    save_merged_report = None

logger = logging.getLogger(__name__)

@dataclass
class ConcreteGrade:
    """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω–æ–π –º–∞—Ä–∫–∏ –±–µ—Ç–æ–Ω–∞"""
    grade: str
    exposure_classes: List[str]
    context: str
    location: str
    confidence: float
    source_document: str
    line_number: Optional[int] = None
    volume_m3: Optional[float] = None
    cost: Optional[float] = None
    method: str = "regex"

class UnifiedConcreteAgent:
    """–ï–¥–∏–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π —Å –±–µ—Ç–æ–Ω–æ–º"""
    
    def __init__(self, knowledge_base_path: str = "knowledge_base/complete-concrete-knowledge-base.json"):
        logger.info("üèóÔ∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è UnifiedConcreteAgent...")
        
        # –ë–∞–∑–æ–≤—ã–µ —Å–µ—Ä–≤–∏—Å—ã
        self.doc_parser = DocParser()
        self.smeta_parser = SmetaParser()
        
        # –£—Ç–∏–ª–∏—Ç—ã –∏ —Å–µ—Ä–≤–∏—Å—ã
        self.knowledge_service = get_knowledge_service()
        self.czech_preprocessor = get_czech_preprocessor()
        self.volume_analyzer = get_volume_analyzer()
        self.report_generator = get_report_generator()
        
        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π volume agent
        self.volume_agent = get_concrete_volume_agent() if get_concrete_volume_agent else None
        
        # LLM —Å–µ—Ä–≤–∏—Å—ã
        self._init_llm_services()
        
        # –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π
        self.allowed_grades = set(self.knowledge_service.get_all_concrete_grades())
        self.context_keywords = list(self.knowledge_service.get_context_keywords())
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.allowed_grades)} –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –º–∞—Ä–æ–∫ –±–µ—Ç–æ–Ω–∞")
        
        # Legacy –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self._load_legacy_knowledge_base(knowledge_base_path)
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
        self._init_patterns()
        
        # –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        self._init_structural_elements()
        
        logger.info("‚úÖ UnifiedConcreteAgent –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
    
    def _init_llm_services(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM —Å–µ—Ä–≤–∏—Å–æ–≤"""
        if CENTRALIZED_LLM_AVAILABLE:
            self.llm_service = get_llm_service()
            self.prompt_loader = get_prompt_loader()
            self.use_centralized_llm = True
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π LLM —Å–µ—Ä–≤–∏—Å")
        else:
            if get_claude_client:
                self.claude_client = get_claude_client()
                self.use_centralized_llm = False
                logger.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è legacy Claude client")
            else:
                self.claude_client = None
                self.use_centralized_llm = False
                logger.warning("‚ö†Ô∏è LLM —Å–µ—Ä–≤–∏—Å—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
    
    def _load_legacy_knowledge_base(self, path: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ legacy –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        try:
            with open(path, "r", encoding="utf-8") as f:
                self.knowledge_base = json.load(f)
            logger.info("üìö Legacy knowledge-base –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å legacy KB: {e}")
            self.knowledge_base = self._create_default_kb()
    
    def _init_patterns(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π"""
        # –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –º–∞—Ä–æ–∫ –±–µ—Ç–æ–Ω–∞
        self.concrete_pattern = r'\b(?:LC|C)\d{1,3}/\d{1,3}\b'
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        self.concrete_patterns = [
            r'(?i)(?:beton(?:u|em|y|ov√©|ov√°)?|bet√≥nov√°?)\s+(?:t≈ô√≠dy?\s+)?((?:LC)?C\d{1,3}/\d{1,3})',
            r'(?i)\b((?:LC)?C\d{1,3}/\d{1,3})\b',
            r'(?i)(?:beton(?:u|em|y)?)\s+(?:t≈ô√≠dy?\s+)?(B\d{1,2})',
            r'(?i)\b(B\d{1,2})\b(?=\s|$|[,.])',
        ]
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è
        self.exposure_patterns = [
            r'\b(X[CDFASM]\d*)\b',
            r'\b(XO)\b'
        ]
    
    def _init_structural_elements(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ–≤–∞—Ä—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        self.structural_elements = {
            'OPƒöRA': {'en': 'abutment', 'applications': ['XC4', 'XD1', 'XF2']},
            'PIL√ç≈ò': {'en': 'pier', 'applications': ['XC4', 'XD3', 'XF4']},
            '≈ò√çMSA': {'en': 'cornice', 'applications': ['XC4', 'XD3', 'XF4']},
            'MOSTOVKA': {'en': 'deck', 'applications': ['XC4', 'XD3', 'XF4']},
            'Z√ÅKLAD': {'en': 'foundation', 'applications': ['XC1', 'XC2', 'XA1']},
            'VOZOVKA': {'en': 'roadway', 'applications': ['XF2', 'XF4', 'XD1']},
            'STƒöNA': {'en': 'wall', 'applications': ['XC1', 'XC3', 'XC4']},
            'DESKA': {'en': 'slab', 'applications': ['XC1', 'XC2']},
        }
    
    def _create_default_kb(self) -> Dict:
        """–°–æ–∑–¥–∞–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        return {
            "strength_classes": {
                "standard": {
                    "C12/15": {"fck": 12, "fcm": 20},
                    "C16/20": {"fck": 16, "fcm": 24},
                    "C20/25": {"fck": 20, "fcm": 28},
                    "C25/30": {"fck": 25, "fcm": 33},
                    "C30/37": {"fck": 30, "fcm": 38},
                }
            }
        }
    
    def _parse_document(self, doc_path: str) -> str:
        """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º doc_parser.parse() –Ω–∞–ø—Ä—è–º—É—é
            text = self.doc_parser.parse(doc_path)
            if text and text.strip():
                return text
            else:
                logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è {doc_path}")
                return ""
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {doc_path}: {e}")
            return ""
    
    def _extract_grades_from_text(self, text: str, source_document: str) -> List[ConcreteGrade]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–∞—Ä–∫–∏ –±–µ—Ç–æ–Ω–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return []
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—à—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        preprocessed = self.czech_preprocessor.preprocess_document_text(text)
        fixed_text = preprocessed['fixed_text']
        logger.info(f"üá®üáø –ü—Ä–∏–º–µ–Ω–µ–Ω–æ {preprocessed['changes_count']} –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–∏–∞–∫—Ä–∏—Ç–∏–∫–∏")
        
        grades = []
        lines = fixed_text.split('\n')
        
        for line_num, line in enumerate(lines):
            # –ü–æ–∏—Å–∫ –º–∞—Ä–æ–∫ –±–µ—Ç–æ–Ω–∞
            for match in re.finditer(self.concrete_pattern, line, re.IGNORECASE):
                grade = self._normalize_concrete_grade(match.group())
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è –º–∞—Ä–∫–∏
                if not self._is_valid_grade(grade):
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                context = self._get_line_context(lines, line_num)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª–∞—Å—Å—ã –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è
                exposure_classes = self._extract_exposure_classes(context)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç
                location = self.czech_preprocessor.identify_construction_element_enhanced(context)
                
                # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –º–∞—Ä–∫–∏
                concrete_grade = ConcreteGrade(
                    grade=grade,
                    exposure_classes=exposure_classes,
                    context=context[:200],
                    location=location,
                    confidence=self._calculate_confidence(grade, context),
                    source_document=source_document,
                    line_number=line_num + 1,
                    method='regex_enhanced'
                )
                
                grades.append(concrete_grade)
        
        return grades
    
    def _normalize_concrete_grade(self, grade: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ –º–∞—Ä–∫–∏ –±–µ—Ç–æ–Ω–∞"""
        if not grade:
            return ""
        return grade.upper().strip().replace(" ", "")
    
    def _is_valid_grade(self, grade: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –º–∞—Ä–∫–∏"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞
        if not re.match(r'^(?:LC|C)\d{1,3}/\d{1,3}$', grade):
            if not re.match(r'^B\d{1,2}$', grade):
                return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        return grade in self.allowed_grades or self.knowledge_service.is_valid_concrete_grade(grade)
    
    def _extract_exposure_classes(self, context: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª–∞—Å—Å—ã –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è"""
        classes = []
        for pattern in self.exposure_patterns:
            matches = re.findall(pattern, context)
            classes.extend(matches)
        return list(set(classes))
    
    def _calculate_confidence(self, grade: str, context: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –º–∞—Ä–∫–µ"""
        confidence = 0.5
        
        if grade in self.allowed_grades:
            confidence += 0.3
        
        context_lower = context.lower()
        for keyword in self.context_keywords:
            if keyword in context_lower:
                confidence += 0.1
                break
        
        if self._extract_exposure_classes(context):
            confidence += 0.1
        
        return min(confidence, 1.0)
    
    def _get_line_context(self, lines: List[str], line_num: int, window: int = 2) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Å—Ç—Ä–æ–∫–∏"""
        start = max(0, line_num - window)
        end = min(len(lines), line_num + window + 1)
        return ' '.join(lines[start:end]).strip()
    
    async def analyze(self, doc_paths: List[str], 
                     smeta_path: Optional[str] = None,
                     use_llm: bool = False,
                     include_volumes: bool = False) -> Dict[str, Any]:
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            doc_paths: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            smeta_path: –ü—É—Ç—å –∫ —Å–º–µ—Ç–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            use_llm: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ LLM –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞
            include_volumes: –í–∫–ª—é—á–∏—Ç—å –ª–∏ –∞–Ω–∞–ª–∏–∑ –æ–±—ä–µ–º–æ–≤
        """
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞: {len(doc_paths)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        all_grades = []
        all_text = ""
        processed_docs = []
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        for doc_path in doc_paths:
            try:
                text = self._parse_document(doc_path)
                if text:
                    all_text += text + "\n"
                    grades = self._extract_grades_from_text(text, doc_path)
                    all_grades.extend(grades)
                    
                    processed_docs.append({
                        'file': Path(doc_path).name,
                        'text_length': len(text),
                        'grades_found': len(grades)
                    })
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {doc_path}: {e}")
                processed_docs.append({
                    'file': Path(doc_path).name,
                    'error': str(e)
                })
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–º–µ—Ç—ã
        smeta_data = []
        if smeta_path:
            try:
                smeta_data = self.smeta_parser.parse(smeta_path)
                if smeta_data:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–∞—Ä–∫–∏ –∏–∑ —Å–º–µ—Ç—ã
                    smeta_text = self._extract_text_from_smeta(smeta_data)
                    if smeta_text:
                        smeta_grades = self._extract_grades_from_text(smeta_text, smeta_path)
                        all_grades.extend(smeta_grades)
                    logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(smeta_data)} –ø–æ–∑–∏—Ü–∏–π —Å–º–µ—Ç—ã")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–º–µ—Ç—ã: {e}")
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –º–∞—Ä–æ–∫
        unique_grades = self._deduplicate_grades(all_grades)
        
        # –ë–∞–∑–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            'success': True,
            'concrete_summary': [self._grade_to_dict(g) for g in unique_grades],
            'total_grades_found': len(unique_grades),
            'unique_grades': list(set(g.grade for g in unique_grades)),
            'processed_documents': processed_docs,
            'analysis_method': 'local_enhanced',
            'smeta_items': len(smeta_data)
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑ –æ–±—ä–µ–º–æ–≤ –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ
        if include_volumes and self.volume_agent:
            try:
                volumes = await self.volume_agent.analyze_volumes_from_documents(doc_paths, smeta_path)
                result['volumes'] = volumes
                result['volume_summary'] = self.volume_agent.create_volume_summary(volumes)
                logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ –æ–±—ä–µ–º–æ–≤: {len(volumes)}")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –æ–±—ä–µ–º–æ–≤: {e}")
        
        # –£–ª—É—á—à–µ–Ω–∏–µ —á–µ—Ä–µ–∑ LLM –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ
        if use_llm and self._is_llm_available():
            try:
                llm_result = await self._enhance_with_llm(all_text, smeta_data)
                if llm_result.get('success'):
                    result['llm_enhancement'] = llm_result
                    result['analysis_method'] = 'hybrid_llm_enhanced'
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ LLM –∞–Ω–∞–ª–∏–∑–∞: {e}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è
        if save_merged_report:
            try:
                save_merged_report(result, "outputs/concrete_analysis_report.json")
                logger.info("üíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á–µ—Ç: {e}")
        
        return result
    
    def _extract_text_from_smeta(self, smeta_data: List[Dict]) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å–º–µ—Ç—ã"""
        text_parts = []
        for row in smeta_data:
            if isinstance(row, dict):
                desc = row.get('description', '')
                if desc:
                    text_parts.append(desc)
        return '\n'.join(text_parts)
    
    def _deduplicate_grades(self, grades: List[ConcreteGrade]) -> List[ConcreteGrade]:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –º–∞—Ä–æ–∫"""
        seen = {}
        for grade in grades:
            key = f"{grade.grade}_{grade.location}"
            if key not in seen or grade.confidence > seen[key].confidence:
                seen[key] = grade
        return list(seen.values())
    
    def _grade_to_dict(self, grade: ConcreteGrade) -> Dict[str, Any]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –æ–±—ä–µ–∫—Ç –º–∞—Ä–∫–∏ –≤ —Å–ª–æ–≤–∞—Ä—å"""
        return {
            'grade': grade.grade,
            'exposure_classes': grade.exposure_classes,
            'location': grade.location,
            'context': grade.context,
            'confidence': round(grade.confidence, 2),
            'source_document': grade.source_document,
            'line_number': grade.line_number,
            'method': grade.method
        }
    
    def _is_llm_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å LLM"""
        if self.use_centralized_llm:
            return self.llm_service is not None
        else:
            return self.claude_client is not None
    
    async def _enhance_with_llm(self, text: str, smeta_data: List[Dict]) -> Dict[str, Any]:
        """–£–ª—É—á—à–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ —á–µ—Ä–µ–∑ LLM"""
        if self.use_centralized_llm:
            return await self._centralized_llm_analysis(text, smeta_data)
        elif self.claude_client:
            return await self._legacy_claude_analysis(text, smeta_data)
        return {'success': False, 'error': 'No LLM available'}
    
    async def _centralized_llm_analysis(self, text: str, smeta_data: List[Dict]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π LLM —Å–µ—Ä–≤–∏—Å"""
        try:
            system_prompt = self.prompt_loader.get_system_prompt("concrete")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            user_prompt = f"""
            Analyze this construction document for concrete grades and specifications.
            Find all concrete grades (C25/30, LC20/25, etc.) with:
            - Exposure classes (XC, XF, XD, etc.)
            - Structural elements
            - Volumes if mentioned
            
            Document text (first 5000 chars):
            {text[:5000]}
            
            Return JSON with found concrete grades.
            """
            
            response = await self.llm_service.run_prompt(
                provider='claude',
                prompt=user_prompt,
                system_prompt=system_prompt,
                model='sonnet'
            )
            
            return response
            
        except Exception as e:
            logger.error(f"‚ùå Centralized LLM error: {e}")
            return {'success': False, 'error': str(e)}
    
    async def _legacy_claude_analysis(self, text: str, smeta_data: List[Dict]) -> Dict[str, Any]:
        """Legacy –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Claude client"""
        try:
            result = await self.claude_client.analyze_concrete_with_claude(text, smeta_data)
            return {
                'success': True,
                'concrete_grades': result.get('claude_analysis', {}).get('concrete_grades', [])
            }
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def create_summary(self, grades: List[ConcreteGrade]) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –º–∞—Ä–∫–∞–º"""
        if not grades:
            return {
                'total_found': 0,
                'unique_grades': [],
                'by_document': {},
                'by_element': {}
            }
        
        unique = list(set(g.grade for g in grades))
        
        by_doc = {}
        for g in grades:
            doc = g.source_document
            if doc not in by_doc:
                by_doc[doc] = []
            by_doc[doc].append(g.grade)
        
        by_element = {}
        for g in grades:
            elem = g.location or 'Unknown'
            if elem not in by_element:
                by_element[elem] = []
            by_element[elem].append(g.grade)
        
        return {
            'total_found': len(grades),
            'unique_grades': unique,
            'by_document': by_doc,
            'by_element': by_element
        }
    
    def save_comprehensive_reports(self, analysis_result: Dict[str, Any], 
                                   output_dir: str = "outputs", 
                                   language: str = "cz") -> Dict[str, str]:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ –≤–∏–¥—ã –æ—Ç—á–µ—Ç–æ–≤
        """
        saved_files = {}
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç—á–µ—Ç–æ–≤
        generator = get_report_generator(language)
        
        # Markdown –æ—Ç—á–µ—Ç
        markdown_file = output_path / f"concrete_analysis_report_{language}.md"
        try:
            if hasattr(generator, 'save_markdown_report') and generator.save_markdown_report(analysis_result, str(markdown_file)):
                saved_files['markdown'] = str(markdown_file)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å Markdown –æ—Ç—á–µ—Ç: {e}")
        
        # JSON –¥–∞–Ω–Ω—ã–µ –¥–ª—è Excel
        json_file = output_path / f"concrete_analysis_data_{language}.json"
        try:
            if hasattr(generator, 'save_json_data') and generator.save_json_data(analysis_result, str(json_file)):
                saved_files['json'] = str(json_file)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å JSON –¥–∞–Ω–Ω—ã–µ: {e}")
        
        # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π JSON –æ—Ç—á–µ—Ç
        json_report_file = output_path / "concrete_analysis_report.json"
        if save_merged_report:
            try:
                save_merged_report(analysis_result, str(json_report_file))
                saved_files['json_report'] = str(json_report_file)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å JSON –æ—Ç—á–µ—Ç: {e}")
        
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(saved_files)} —Ñ–∞–π–ª–æ–≤ –æ—Ç—á–µ—Ç–æ–≤")
        return saved_files


# ==============================
# üîß –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
# ==============================

_concrete_agent = None

def get_concrete_agent() -> UnifiedConcreteAgent:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –∞–≥–µ–Ω—Ç–∞"""
    global _concrete_agent
    if _concrete_agent is None:
        _concrete_agent = UnifiedConcreteAgent()
        logger.info("‚úÖ UnifiedConcreteAgent –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (singleton)")
    return _concrete_agent

# –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å ConcreteAgentHybrid
def get_hybrid_agent() -> UnifiedConcreteAgent:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –∞–≥–µ–Ω—Ç–∞ (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)"""
    return get_concrete_agent()

# ==============================
# üöÄ API —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
# ==============================

async def analyze_concrete(doc_paths: List[str], 
                          smeta_path: Optional[str] = None,
                          use_claude: bool = True,
                          claude_mode: str = "enhancement") -> Dict[str, Any]:
    """
    API —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    –ú–∞–ø–∏—Ç —Å—Ç–∞—Ä—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ –Ω–æ–≤—ã–µ
    """
    agent = get_hybrid_agent()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω—É—é —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
    
    # –ú–∞–ø–∏–Ω–≥ —Å—Ç–∞—Ä—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    use_llm = use_claude
    include_volumes = True  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤–∫–ª—é—á–∞–µ–º –æ–±—ä–µ–º—ã
    
    result = await agent.analyze(
        doc_paths=doc_paths,
        smeta_path=smeta_path,
        use_llm=use_llm,
        include_volumes=include_volumes
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è
    if save_merged_report:
        try:
            save_merged_report(result, "outputs/concrete_analysis_report.json")
            logger.info("üíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ outputs/concrete_analysis_report.json")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á—ë—Ç: {e}")
    
    return result

async def analyze_concrete_with_volumes(doc_paths: List[str],
                                       smeta_path: Optional[str] = None,
                                       use_claude: bool = True,
                                       claude_mode: str = "enhancement",
                                       language: str = "cz") -> Dict[str, Any]:
    """
    API —Ñ—É–Ω–∫—Ü–∏—è —Å –æ–±—ä–µ–º–∞–º–∏ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    """
    agent = get_hybrid_agent()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω—É—é —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —è–∑—ã–∫ –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤
    agent.report_generator = get_report_generator(language)
    
    result = await agent.analyze(
        doc_paths=doc_paths,
        smeta_path=smeta_path,
        use_llm=use_claude,
        include_volumes=True
    )
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —è–∑—ã–∫–∞
    result['report_language'] = language
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã —Ñ—É–Ω–∫—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    if hasattr(agent, 'save_comprehensive_reports'):
        try:
            saved_files = agent.save_comprehensive_reports(result, "outputs", language)
            result['saved_reports'] = saved_files
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            concrete_count = len(result.get('concrete_summary', []))
            volume_count = result.get('volume_entries_found', 0)
            
            logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω:")
            logger.info(f"   - –ù–∞–π–¥–µ–Ω–æ –º–∞—Ä–æ–∫ –±–µ—Ç–æ–Ω–∞: {concrete_count}")
            logger.info(f"   - –ù–∞–π–¥–µ–Ω–æ –æ–±—ä–µ–º–æ–≤: {volume_count}")
            logger.info(f"   - –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –æ—Ç—á–µ—Ç–æ–≤: {len(saved_files)}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á–µ—Ç—ã: {e}")
    
    return result

# ==============================
# üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
# ==============================

async def test_unified_agent():
    """–¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ UnifiedConcreteAgent")
    print("=" * 50)
    
    agent = get_concrete_agent()
    
    # –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç
    test_text = """
    MOSTN√ç OPƒöRA ZE ≈ΩELEZOBETONU C30/37 XD2 XF1
    Pou≈æit√Ω beton: C25/30 XC2
    Z√°kladov√° deska - monolitick√Ω beton C20/25
    """
    
    # –¢–µ—Å—Ç–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
    grades = agent._extract_grades_from_text(test_text, "test.pdf")
    summary = agent.create_summary(grades)
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –º–∞—Ä–æ–∫: {summary['total_found']}")
    print(f"üìã –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –º–∞—Ä–∫–∏: {summary['unique_grades']}")
    
    for grade in grades:
        print(f"\n  ‚Ä¢ {grade.grade}")
        print(f"    –ö–ª–∞—Å—Å—ã: {', '.join(grade.exposure_classes)}")
        print(f"    –≠–ª–µ–º–µ–Ω—Ç: {grade.location}")
        print(f"    –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {grade.confidence:.2%}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(test_unified_agent())