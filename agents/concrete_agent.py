"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π ConcreteAgentHybrid ‚Äî –∞–≥–µ–Ω—Ç –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –±–µ—Ç–æ–Ω–æ–≤.
"""

import re
import os
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass

from parsers.doc_parser import DocParser
from parsers.smeta_parser import SmetaParser
from utils.claude_client import get_claude_client
from config.settings import settings
from outputs.save_report import save_merged_report

logger = logging.getLogger(__name__)

@dataclass
class ConcreteMatch:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω–æ–π –º–∞—Ä–∫–∏ –±–µ—Ç–æ–Ω–∞"""
    grade: str
    context: str
    location: str
    confidence: float
    method: str
    coordinates: Optional[Tuple[int, int, int, int]] = None

@dataclass 
class StructuralElement:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞"""
    name: str
    concrete_grade: Optional[str]
    location: str
    context: str


class ConcreteAgentHybrid:
    def __init__(self, knowledge_base_path="knowledge_base/complete-concrete-knowledge-base.json"):
        self.kb_path = knowledge_base_path
        self.knowledge_base = self._load_knowledge_base()
        self.allowed_grades = set(self._extract_valid_grades_from_kb(self.knowledge_base))

    def _load_knowledge_base(self) -> Dict:
        with open(self.kb_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def _extract_valid_grades_from_kb(self, kb: Dict) -> List[str]:
        grades = []
        grades.extend(kb.get("strength_classes", {}).get("standard", {}).keys())
        grades.extend(kb.get("strength_classes", {}).get("uhpc", {}).keys())
        grades.extend(kb.get("concrete_types_by_density", {}).get("lehk√Ω_beton", {}).get("strength_classes", []))
        return grades

    def _normalize_concrete_grade(self, raw: str) -> str:
        return raw.upper().replace(" ", "").strip()

    def _local_concrete_analysis(self, text: str) -> List[ConcreteMatch]:
        results = []
        pattern = r'\b(?:LC|C)\d{1,3}/\d{1,3}\b'
        matches = re.finditer(pattern, text)
        for match in matches:
            grade = self._normalize_concrete_grade(match.group())
            if grade not in self.allowed_grades:
                continue
            context_window = 50
            start, end = match.start(), match.end()
            context = text[max(0, start - context_window):min(len(text), end + context_window)]
            if not re.search(r'\b(beton(u|em|y)?|t≈ô√≠da|stupe≈à|XC\d|XF\d|XD\d|XA\d|XM\d)\b', context, re.IGNORECASE):
                continue
            if len(grade) > 8 or '/' not in grade or re.match(r'C\d{4,}', grade):
                continue
            results.append(ConcreteMatch(
                grade=grade,
                context=context,
                location="text",
                confidence=0.95,
                method="regex"
            ))
        return results

    def __init__(self, knowledge_base_path="knowledge_base/complete-concrete-knowledge-base.json"):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        try:
            with open(knowledge_base_path, "r", encoding="utf-8") as f:
                self.knowledge_base = json.load(f)
            logger.info("üìö Knowledge-base –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å knowledge-base: {e}")
            self.knowledge_base = self._create_default_kb()

        self.doc_parser = DocParser()
        self.smeta_parser = SmetaParser()
        self.claude_client = get_claude_client() if settings.is_claude_enabled() else None

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –º–∞—Ä–æ–∫ –±–µ—Ç–æ–Ω–∞
        self.concrete_patterns = [
            r'\bC\d{1,2}/\d{1,2}(?:\s*[-‚Äì]\s*[XO][CDFASM]?\d*(?:\s*,\s*[XO][CDFASM]?\d*)*)*\b',
            r'\b[BC]\s*\d{1,2}(?:/\d{1,2})?\b',
            r'\bLC\s*\d{1,2}/\d{1,2}\b',
            r'(?i)(?:beton[u√°]?\s+)?(?:t≈ô√≠d[ayƒõ]\s+)?(\d{2}/\d{2}|\d{2,3})\b',
            r'(?i)(?:betony?|betonov√©[j]?)\s+(?:t≈ô√≠d[yƒõ]\s+)?([BC]?\d{1,2}(?:/\d{1,2})?)',
            r'\b(?:vysokopevnostn[√≠√Ω]|lehk[√Ω√°]|tƒõ≈æk[√Ω√°])\s+beton\s+([BC]?\d{1,2}(?:/\d{1,2})?)\b',
        ]

        # –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (—á–µ—à—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã)
        self.structural_elements = {
            'OPƒöRA': {'en': 'abutment', 'applications': ['XC4', 'XD1', 'XF2']},
            'PIL√ç≈ò': {'en': 'pier', 'applications': ['XC4', 'XD3', 'XF4']},
            '≈ò√çMSA': {'en': 'cornice', 'applications': ['XC4', 'XD3', 'XF4']},
            'MOSTOVKA': {'en': 'deck', 'applications': ['XC4', 'XD3', 'XF4']},
            'Z√ÅKLAD': {'en': 'foundation', 'applications': ['XC1', 'XC2', 'XA1']},
            'VOZOVKA': {'en': 'roadway', 'applications': ['XF2', 'XF4', 'XD1']},
            'GAR√Å≈Ω': {'en': 'garage', 'applications': ['XD1']},
            'STƒöNA': {'en': 'wall', 'applications': ['XC1', 'XC3', 'XC4']},
            'SLOUP': {'en': 'column', 'applications': ['XC1', 'XC3']},
            'SCHODI≈†Tƒö': {'en': 'stairs', 'applications': ['XC1', 'XC3']},
            'PODKLADN√ç': {'en': 'subgrade', 'applications': ['X0', 'XC1']},
            'NOSN√Å': {'en': 'load-bearing', 'applications': ['XC1', 'XC3']},
            'VƒöNEC': {'en': 'tie-beam', 'applications': ['XC2', 'XF1']},
            'DESKA': {'en': 'slab', 'applications': ['XC1', 'XC2']},
            'PREFABRIK√ÅT': {'en': 'precast', 'applications': ['XC1', 'XC3']},
            'MONOLITICK√ù': {'en': 'monolithic', 'applications': ['XC1', 'XC2']},
        }

        # –ö–æ–¥—ã —Å–º–µ—Ç –∏ –∏—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
        self.smeta_codes = {
            '801': 'Prost√Ω beton',
            '802': '≈Ωelezobeton monolitick√Ω',
            '803': '≈Ωelezobeton prefabrikovan√Ω',
            '811': 'Betony speci√°ln√≠',
            '271': 'Konstrukce betonov√© a ≈æelezobetonov√©',
            'HSV': 'Hlavn√≠ stavebn√≠ v√Ωroba',
            'PSV': 'P≈ôidru≈æen√° stavebn√≠ v√Ωroba',
        }

    def _create_default_kb(self) -> Dict:
        """–°–æ–∑–¥–∞–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        return {
            "environment_classes": {
                "XC1": {"description": "such√© nebo st√°le mokr√©", "applications": ["interi√©r", "z√°klady"]},
                "XC2": {"description": "mokr√©, obƒças such√©", "applications": ["z√°klady", "spodn√≠ stavba"]},
                "XC3": {"description": "st≈ôednƒõ mokr√©", "applications": ["kryt√© prostory"]},
                "XC4": {"description": "st≈ô√≠davƒõ mokr√© a such√©", "applications": ["vnƒõj≈°√≠ povrchy"]},
                "XD1": {"description": "chloridy - m√≠rn√©", "applications": ["gar√°≈æe", "parkovi≈°tƒõ"]},
                "XD3": {"description": "chloridy - siln√©", "applications": ["mosty", "vozovky"]},
                "XF1": {"description": "mr√°z - m√≠rn√Ω", "applications": ["vnƒõj≈°√≠ svisl√© plochy"]},
                "XF2": {"description": "mr√°z + soli", "applications": ["silniƒçn√≠ konstrukce"]},
                "XF4": {"description": "mr√°z + soli - extr√©mn√≠", "applications": ["mostovky", "vozovky"]},
                "XA1": {"description": "chemicky agresivn√≠ - slabƒõ", "applications": ["z√°klady", "septiky"]},
            }
        }

    def _local_concrete_analysis(self, text: str) -> Dict[str, Any]:
        """–õ–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –º–∞—Ä–æ–∫ –±–µ—Ç–æ–Ω–∞"""
        all_matches = []
        
        for pattern in self.concrete_patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                grade = self._normalize_concrete_grade(match.group().strip())
                context = text[max(0, match.start()-100):match.end()+100]
                location = self._identify_structural_element(context)
                
                all_matches.append(ConcreteMatch(
                    grade=grade,
                    context=context.strip(),
                    location=location,
                    confidence=0.9,
                    method='regex',
                    coordinates=None
                ))
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        unique_matches = {}
        for match in all_matches:
            key = match.grade
            if key not in unique_matches or match.confidence > unique_matches[key].confidence:
                unique_matches[key] = match
        
        return {
            'concrete_summary': [
                {
                    'grade': match.grade,
                    'location': match.location,
                    'context': match.context[:200],
                    'confidence': match.confidence,
                    'method': match.method
                }
                for match in unique_matches.values()
            ],
            'analysis_method': 'local',
            'total_matches': len(unique_matches),
            'success': True
        }

    def _normalize_concrete_grade(self, grade: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ –º–∞—Ä–∫–∏ –±–µ—Ç–æ–Ω–∞"""
        normalized = re.sub(r'\s+', '', grade.upper())
        normalized = re.sub(r'^B(\d+)$', r'C\1/\1', normalized)  # B20 -> C20/25
        normalized = re.sub(r'^(\d+)$', r'C\1', normalized)       # 30 -> C30
        return normalized

    def _identify_structural_element(self, context: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        context_upper = context.upper()
        
        for element, info in self.structural_elements.items():
            if element in context_upper:
                return f"{element} ({info['en']})"
        
        for element, info in self.structural_elements.items():
            if any(part in context_upper for part in element.split()):
                return f"{element} ({info['en']}) - —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ"
        
        return "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ"

    async def _claude_concrete_analysis(self, text: str, smeta_data: List[Dict]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Claude —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        if not self.claude_client:
            return {"success": False, "error": "Claude client not available"}
        
        try:
            result = await self.claude_client.analyze_concrete_with_claude(text, smeta_data)
            
            return {
                'concrete_summary': result.get('claude_analysis', {}).get('concrete_grades', []),
                'analysis_method': 'claude_enhanced',
                'success': True,
                'tokens_used': result.get('tokens_used', 0),
                'raw_response': result.get('raw_response', '')
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Claude –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return {"success": False, "error": str(e)}

    async def analyze(self, doc_paths: List[str], smeta_path: Optional[str] = None,
                      use_claude: bool = True, claude_mode: str = "enhancement") -> Dict[str, Any]:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
        """
        logger.info(f"üèóÔ∏è –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (—Ä–µ–∂–∏–º: {claude_mode})")
        
        all_text = ""
        processed_docs = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        for doc_path in doc_paths:
            try:
                text = self.doc_parser.parse(doc_path)
                all_text += text + "\n"
                
                processed_docs.append({
                    'file': Path(doc_path).name,
                    'type': 'Document',
                    'text_length': len(text)
                })
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {doc_path}: {e}")
                processed_docs.append({'file': Path(doc_path).name, 'error': str(e)})
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–º–µ—Ç—É
        smeta_data = []
        if smeta_path:
            try:
                smeta_result = self.smeta_parser.parse(smeta_path)
                smeta_data = smeta_result.get('items', [])
                logger.info(f"üìä –°–º–µ—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞: {len(smeta_data)} –ø–æ–∑–∏—Ü–∏–π")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–º–µ—Ç—ã: {e}")
        
        # –õ–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        local_result = self._local_concrete_analysis(all_text)
        
        # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Claude
        final_result = local_result.copy()
        
        if use_claude and self.claude_client:
            claude_result = await self._claude_concrete_analysis(all_text, smeta_data)
            
            if claude_mode == "primary" and claude_result.get("success"):
                final_result = claude_result
            elif claude_mode == "enhancement" and claude_result.get("success"):
                final_result.update({
                    'claude_analysis': claude_result,
                    'analysis_method': 'hybrid_enhanced',
                    'total_tokens_used': claude_result.get('tokens_used', 0)
                })
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        final_result.update({
            'processed_documents': processed_docs,
            'smeta_items': len(smeta_data),
            'processing_time': 'completed',
            'knowledge_base_version': '3.0'
        })
        
        return final_result

# ==============================
# üîß –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ==============================

_hybrid_agent = None

def get_hybrid_agent() -> ConcreteAgentHybrid:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞"""
    global _hybrid_agent
    if _hybrid_agent is None:
        _hybrid_agent = ConcreteAgentHybrid()
        logger.info("ü§ñ ConcreteAgentHybrid –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    return _hybrid_agent

# ==============================
# üöÄ API-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ==============================

async def analyze_concrete(doc_paths: List[str], smeta_path: Optional[str] = None,
                           use_claude: bool = True, claude_mode: str = "enhancement") -> Dict[str, Any]:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–µ—Ç–æ–Ω–∞ - —Å–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º API
    """
    agent = get_hybrid_agent()
    
    try:
        result = await agent.analyze(doc_paths, smeta_path, use_claude, claude_mode)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        try:
            save_merged_report(result, "outputs/concrete_analysis_report.json")
            logger.info("üíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ outputs/concrete_analysis_report.json")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á—ë—Ç: {e}")
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ analyze_concrete: {e}")
        return {
            "error": str(e),
            "success": False,
            "analysis_method": "error",
            "concrete_summary": []
        }
