"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π ConcreteAgentHybrid ‚Äî –∞–≥–µ–Ω—Ç –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –±–µ—Ç–æ–Ω–æ–≤.
–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- OCR –¥–ª—è —á–µ—Ä—Ç–µ–∂–µ–π —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ–º –º–∞—Ä–æ–∫ –∏ –º–µ—Å—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
- –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ regex –¥–ª—è –≤—Å–µ—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å knowledge base
- –ü–∞—Ä—Å–∏–Ω–≥ XML —Å–º–µ—Ç —Å –∫–æ–¥–∞–º–∏
- –£–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ Claude
"""

import re
import os
import json
import logging
import pdfplumber
import pytesseract
import cv2
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from xml.etree import ElementTree as ET
from PIL import Image, ImageEnhance
from dataclasses import dataclass

from parsers.doc_parser import DocParser
from parsers.smeta_parser import SmetaParser
from utils.claude_client import get_claude_client
from config.settings import settings
from outputs.save_report import save_merged_report

logger = logging.getLogger(__name__)

@dataclass
class ConcreteMatch:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω–æ–π –º–∞—Ä–∫–∏ –±–µ—Ç–æ–Ω–∞"""
    grade: str
    context: str
    location: str
    confidence: float
    method: str
    coordinates: Optional[Tuple[int, int, int, int]] = None

@dataclass 
class StructuralElement:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞"""
    name: str
    concrete_grade: Optional[str]
    location: str
    context: str

class analyze_concrete:
    def __init__(self, knowledge_base_path="complete-concrete-knowledge-base.json"):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        try:
            with open(knowledge_base_path, "r", encoding="utf-8") as f:
                self.knowledge_base = json.load(f)
            logger.info("üìö Knowledge-base –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å knowledge-base: {e}")
            self.knowledge_base = self._create_default_kb()

        self.doc_parser = DocParser()
        self.smeta_parser = SmetaParser()
        self.claude_client = get_claude_client() if settings.is_claude_enabled() else None

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –º–∞—Ä–æ–∫ –±–µ—Ç–æ–Ω–∞
        self.concrete_patterns = [
            # –ü–æ–ª–Ω—ã–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è —Å –∫–ª–∞—Å—Å–∞–º–∏ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è
            r'\bC\d{1,2}/\d{1,2}(?:\s*[-‚Äì]\s*[XO][CDFASM]?\d*(?:\s*,\s*[XO][CDFASM]?\d*)*)*\b',
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è
            r'\b[BC]\s*\d{1,2}(?:/\d{1,2})?\b',
            # –õ–µ–≥–∫–∏–π –±–µ—Ç–æ–Ω
            r'\bLC\s*\d{1,2}/\d{1,2}\b',
            # –¢–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã (–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ)
            r'(?i)(?:beton[u√°]?\s+)?(?:t≈ô√≠d[ayƒõ]\s+)?(\d{2}/\d{2}|\d{2,3})\b',
            # –í —á–µ—à—Å–∫–æ–º —Ç–µ–∫—Å—Ç–µ
            r'(?i)(?:betony?|betonov√©j?)\s+(?:t≈ô√≠d[yƒõ]\s+)?([BC]?\d{1,2}(?:/\d{1,2})?)',
            # –° –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            r'\b(?:vysokopevnostn[√≠√Ω]|lehk[√Ω√°]|tƒõ≈æk[√Ω√°])\s+beton\s+([BC]?\d{1,2}(?:/\d{1,2})?)\b',
        ]

        # –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (—á–µ—à—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã)
        self.structural_elements = {
            # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
            'OPƒöRA': {'en': 'abutment', 'applications': ['XC4', 'XD1', 'XF2']},
            'PIL√ç≈ò': {'en': 'pier', 'applications': ['XC4', 'XD3', 'XF4']},
            '≈ò√çMSA': {'en': 'cornice', 'applications': ['XC4', 'XD3', 'XF4']},
            'MOSTOVKA': {'en': 'deck', 'applications': ['XC4', 'XD3', 'XF4']},
            'Z√ÅKLAD': {'en': 'foundation', 'applications': ['XC1', 'XC2', 'XA1']},
            'VOZOVKA': {'en': 'roadway', 'applications': ['XF2', 'XF4', 'XD1']},
            'GAR√Å≈Ω': {'en': 'garage', 'applications': ['XD1']},
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            'STƒöNA': {'en': 'wall', 'applications': ['XC1', 'XC3', 'XC4']},
            'SLOUP': {'en': 'column', 'applications': ['XC1', 'XC3']},
            'SCHODI≈†Tƒö': {'en': 'stairs', 'applications': ['XC1', 'XC3']},
            'PODKLADN√ç': {'en': 'subgrade', 'applications': ['X0', 'XC1']},
            'NOSN√Å': {'en': 'load-bearing', 'applications': ['XC1', 'XC3']},
            'VƒöNEC': {'en': 'tie-beam', 'applications': ['XC2', 'XF1']},
            'DESKA': {'en': 'slab', 'applications': ['XC1', 'XC2']},
            'PREFABRIK√ÅT': {'en': 'precast', 'applications': ['XC1', 'XC3']},
            'MONOLITICK√ù': {'en': 'monolithic', 'applications': ['XC1', 'XC2']},
        }

        # –ö–æ–¥—ã —Å–º–µ—Ç –∏ –∏—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
        self.smeta_codes = {
            '801': 'Prost√Ω beton',
            '802': '≈Ωelezobeton monolitick√Ω',
            '803': '≈Ωelezobeton prefabrikovan√Ω',
            '811': 'Betony speci√°ln√≠',
            '271': 'Konstrukce betonov√© a ≈æelezobetonov√©',
            'HSV': 'Hlavn√≠ stavebn√≠ v√Ωroba',
            'PSV': 'P≈ôidru≈æen√° stavebn√≠ v√Ωroba',
        }

    def _create_default_kb(self) -> Dict:
        """–°–æ–∑–¥–∞–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        return {
            "environment_classes": {
                "XC1": {"description": "such√© nebo st√°le mokr√©", "applications": ["interi√©r", "z√°klady"]},
                "XC2": {"description": "mokr√©, obƒças such√©", "applications": ["z√°klady", "spodn√≠ stavba"]},
                "XC3": {"description": "st≈ôednƒõ mokr√©", "applications": ["kryt√© prostory"]},
                "XC4": {"description": "st≈ô√≠davƒõ mokr√© a such√©", "applications": ["vnƒõj≈°√≠ povrchy"]},
                "XD1": {"description": "chloridy - m√≠rn√©", "applications": ["gar√°≈æe", "parkovi≈°tƒõ"]},
                "XD3": {"description": "chloridy - siln√©", "applications": ["mosty", "vozovky"]},
                "XF1": {"description": "mr√°z - m√≠rn√Ω", "applications": ["vnƒõj≈°√≠ svisl√© plochy"]},
                "XF2": {"description": "mr√°z + soli", "applications": ["silniƒçn√≠ konstrukce"]},
                "XF4": {"description": "mr√°z + soli - extr√©mn√≠", "applications": ["mostovky", "vozovky"]},
                "XA1": {"description": "chemicky agresivn√≠ - slabƒõ", "applications": ["z√°klady", "septiky"]},
            }
        }

    # ==============================
# üîß –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ==============================

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–≥–µ–Ω—Ç–∞
_hybrid_agent = None

def get_hybrid_agent() -> analyze_concrete:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞"""
    global _hybrid_agent
    if _hybrid_agent is None:
        _hybrid_agent = analyze_concrete()
        logger.info("ü§ñ analyze_concrete –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    return _hybrid_agent

# ==============================
# üöÄ API-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ==============================

async def analyze_concrete(doc_paths: List[str], smeta_path: Optional[str] = None,
                           use_claude: bool = True, claude_mode: str = "enhancement") -> Dict[str, Any]:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–µ—Ç–æ–Ω–∞ - —Å–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º API
    
    Args:
        doc_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        smeta_path: –ü—É—Ç—å –∫ —Å–º–µ—Ç–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        use_claude: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ Claude –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        claude_mode: –†–µ–∂–∏–º Claude ("enhancement", "primary", "fallback")
        
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
    """
    agent = get_hybrid_agent()
    
    try:
        result = await agent.analyze(doc_paths, smeta_path, use_claude, claude_mode)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        try:
            save_merged_report(result, "outputs/concrete_analysis_report.json")
            logger.info("üíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ outputs/concrete_analysis_report.json")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á—ë—Ç: {e}")
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ analyze_concrete: {e}")
        return {
            "error": str(e),
            "success": False,
            "analysis_method": "error",
            "concrete_summary": []
        }

def analyze_drawings_ocr(image_paths: List[str]) -> Dict[str, Any]:
    """
    –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —á–µ—Ä—Ç–µ–∂–µ–π —Å OCR
    
    Args:
        image_paths: –ü—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º —á–µ—Ä—Ç–µ–∂–µ–π
        
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ OCR –∞–Ω–∞–ª–∏–∑–∞
    """
    agent = get_hybrid_agent()
    results = []
    
    for image_path in image_paths:
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = Image.open(image_path)
            
            # OCR –∞–Ω–∞–ª–∏–∑
            text, word_positions = agent._extract_text_with_ocr(image)
            
            # –ü–æ–∏—Å–∫ –º–∞—Ä–æ–∫ –±–µ—Ç–æ–Ω–∞
            concrete_matches = agent._find_concrete_in_drawing(text, word_positions)
            
            results.append({
                'image': Path(image_path).name,
                'extracted_text': text[:500],  # –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤
                'concrete_matches': [
                    {
                        'grade': match.grade,
                        'location': match.location,
                        'confidence': match.confidence,
                        'coordinates': match.coordinates
                    } for match in concrete_matches
                ],
                'word_positions_count': len(word_positions)
            })
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ OCR –∞–Ω–∞–ª–∏–∑–∞ {image_path}: {e}")
            results.append({
                'image': Path(image_path).name,
                'error': str(e)
            })
    
    return {
        'ocr_results': results,
        'total_images': len(image_paths),
        'successful_analyses': len([r for r in results if 'error' not in r]),
        'analysis_method': 'ocr_specialized'
    }

def analyze_xml_smeta(xml_path: str) -> Dict[str, Any]:
    """
    –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ XML —Å–º–µ—Ç
    
    Args:
        xml_path: –ü—É—Ç—å –∫ XML —Ñ–∞–π–ª—É —Å–º–µ—Ç—ã
        
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞ —Å–º–µ—Ç—ã
    """
    agent = get_hybrid_agent()
    
    try:
        smeta_items = agent._parse_xml_smeta_advanced(xml_path)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º –±–µ—Ç–æ–Ω–∞
        concrete_summary = {}
        total_volume = 0
        total_cost = 0
        
        for item in smeta_items:
            if 'concrete_grades' in item:
                for grade in item['concrete_grades']:
                    if grade not in concrete_summary:
                        concrete_summary[grade] = {
                            'grade': grade,
                            'total_quantity': 0,
                            'items': [],
                            'unit': item.get('unit', 'm¬≥')
                        }
                    
                    concrete_summary[grade]['items'].append({
                        'description': item.get('description', ''),
                        'quantity': item.get('quantity', 0),
                        'code': item.get('code', ''),
                        'price': item.get('price', 0)
                    })
                    
                    # –°—É–º–º–∏—Ä—É–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                    if isinstance(item.get('quantity'), (int, float)):
                        concrete_summary[grade]['total_quantity'] += item['quantity']
                        total_volume += item['quantity']
                    
                    # –°—É–º–º–∏—Ä—É–µ–º —Å—Ç–æ–∏–º–æ—Å—Ç—å
                    if isinstance(item.get('price'), (int, float)):
                        total_cost += item['price']
        
        return {
            'xml_smeta_analysis': {
                'file': Path(xml_path).name,
                'concrete_summary': list(concrete_summary.values()),
                'total_items': len(smeta_items),
                'concrete_items': len([i for i in smeta_items if 'concrete_grades' in i]),
                'total_concrete_volume': total_volume,
                'estimated_total_cost': total_cost,
                'analysis_method': 'xml_smeta_specialized',
                'success': True
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ XML —Å–º–µ—Ç—ã: {e}")
        return {
            'error': str(e),
            'success': False,
            'analysis_method': 'xml_smeta_error'
        }

def get_knowledge_base_info() -> Dict[str, Any]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
    """
    agent = get_hybrid_agent()
    kb = agent.knowledge_base
    
    return {
        'knowledge_base_status': 'loaded' if kb else 'not_loaded',
        'environment_classes_count': len(kb.get('concrete_knowledge_base', {}).get('environment_classes', {})),
        'structural_elements_count': len(agent.structural_elements),
        'concrete_patterns_count': len(agent.concrete_patterns),
        'smeta_codes_count': len(agent.smeta_codes),
        'supported_features': [
            'OCR for drawings',
            'Advanced PDF parsing', 
            'XML smeta parsing',
            'Knowledge base lookup',
            'Structural element detection',
            'Claude AI enhancement',
            'Multi-format concrete grade detection'
        ],
        'supported_formats': [
            'PDF (text + OCR)',
            'DOCX', 
            'TXT',
            'XML (smet—ã)',
            'Images (PNG, JPG, TIFF)'
        ]
    }

# ==============================
# üß™ –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
# ==============================

def test_concrete_patterns(test_string: str) -> Dict[str, Any]:
    """
    –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≤—Å–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ–∏—Å–∫–∞ –±–µ—Ç–æ–Ω–∞ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ —Å—Ç—Ä–æ–∫–∏
    """
    agent = get_hybrid_agent()
    results = {}
    
    for i, pattern in enumerate(agent.concrete_patterns):
        matches = re.findall(pattern, test_string, re.IGNORECASE)
        results[f'pattern_{i+1}'] = {
            'pattern': pattern,
            'matches': [agent._normalize_concrete_grade(m) for m in matches],
            'count': len(matches)
        }
    
    return {
        'test_string': test_string,
        'pattern_results': results,
        'total_unique_matches': len(set([
            agent._normalize_concrete_grade(m) 
            for pattern_result in results.values() 
            for m in pattern_result['matches']
        ]))
    }

def test_structural_elements(test_string: str) -> Dict[str, Any]:
    """
    –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    """
    agent = get_hybrid_agent()
    detected_elements = []
    
    test_upper = test_string.upper()
    for element, info in agent.structural_elements.items():
        if element in test_upper:
            detected_elements.append({
                'element': element,
                'english': info['en'],
                'applications': info['applications']
            })
    
    return {
        'test_string': test_string,
        'detected_elements': detected_elements,
        'location_identification': agent._identify_structural_element(test_string)
    }

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    import asyncio
    
    async def run_examples():
        # –ü—Ä–∏–º–µ—Ä 1: –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        print("üèóÔ∏è –¢–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        result = await analyze_concrete(
            doc_paths=["example.pdf"], 
            use_claude=False,
            claude_mode="enhancement"
        )
        print(f"–ù–∞–π–¥–µ–Ω–æ –º–∞—Ä–æ–∫ –±–µ—Ç–æ–Ω–∞: {len(result.get('concrete_summary', []))}")
        
        # –ü—Ä–∏–º–µ—Ä 2: –¢–µ—Å—Ç –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        print("\nüîç –¢–µ—Å—Ç –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤...")
        test_result = test_concrete_patterns("Pou≈æit√Ω beton C30/37-XF2, XD1 pro ≈ò√çMSA a PIL√ç≈ò")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {test_result['total_unique_matches']}")
        
        # –ü—Ä–∏–º–µ—Ä 3: –¢–µ—Å—Ç –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        print("\nüè¢ –¢–µ—Å—Ç –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤...")
        struct_result = test_structural_elements("BETONOV√Å OPƒöRA C25/30-XF1")
        print(f"–ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(struct_result['detected_elements'])}")
        
        # –ü—Ä–∏–º–µ—Ä 4: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        print("\nüìö –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π...")
        kb_info = get_knowledge_base_info()
        print(f"–°—Ç–∞—Ç—É—Å: {kb_info['knowledge_base_status']}")
        print(f"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: {', '.join(kb_info['supported_formats'])}")
    
    # –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–æ–≤
    # asyncio.run(run_examples())
    # üñºÔ∏è OCR –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    # ==============================
    def _preprocess_image_for_ocr(self, image: Image.Image) -> Image.Image:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è OCR"""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ numpy array
        img_array = np.array(image)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –≥—Ä–∞–¥–∞—Ü–∏–∏ —Å–µ—Ä–æ–≥–æ
        if len(img_array.shape) == 3:
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        else:
            gray = img_array
        
        # –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç–∏
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        enhanced = clahe.apply(gray)
        
        # –ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
        binary = cv2.adaptiveThreshold(
            enhanced, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
            cv2.THRESH_BINARY, 11, 2
        )
        
        # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —à—É–º–∞
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))
        cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
        
        return Image.fromarray(cleaned)

    def _extract_text_with_ocr(self, image: Image.Image, preprocess: bool = True) -> Tuple[str, Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é OCR"""
        if preprocess:
            image = self._preprocess_image_for_ocr(image)
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Tesseract –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —á–µ—Ä—Ç–µ–∂–µ–π
        config = r'--oem 3 --psm 6 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZƒåƒéƒö≈á≈ò≈†≈§≈Æ≈Ω0123456789/-.,()[]'
        
        try:
            # –û—Å–Ω–æ–≤–Ω–æ–π OCR —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            data = pytesseract.image_to_data(
                image, lang='ces+eng', config=config, output_type=pytesseract.Output.DICT
            )
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            text = pytesseract.image_to_string(image, lang='ces+eng', config=config)
            
            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç —Å–ª–æ–≤
            word_positions = {}
            for i in range(len(data['text'])):
                if int(data['conf'][i]) > 30:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                    word = data['text'][i].strip()
                    if word:
                        word_positions[word] = {
                            'x': data['left'][i],
                            'y': data['top'][i], 
                            'w': data['width'][i],
                            'h': data['height'][i],
                            'conf': data['conf'][i]
                        }
            
            return text, word_positions
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ OCR: {e}")
            return "", {}

    def _find_concrete_in_drawing(self, text: str, word_positions: Dict) -> List[ConcreteMatch]:
        """–ü–æ–∏—Å–∫ –º–∞—Ä–æ–∫ –±–µ—Ç–æ–Ω–∞ –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        matches = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        lines = text.split('\n')
        
        for i, line in enumerate(lines):
            # –ò—â–µ–º –º–∞—Ä–∫–∏ –±–µ—Ç–æ–Ω–∞ –≤ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–µ
            for pattern in self.concrete_patterns:
                concrete_matches = re.finditer(pattern, line, re.IGNORECASE)
                
                for match in concrete_matches:
                    grade = match.group().strip()
                    if not grade:
                        continue
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç (—Å–æ—Å–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏)
                    context_lines = []
                    for j in range(max(0, i-2), min(len(lines), i+3)):
                        context_lines.append(lines[j].strip())
                    context = ' '.join(context_lines)
                    
                    # –ò—â–µ–º –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
                    location = self._identify_structural_element(context)
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
                    coordinates = self._find_word_coordinates(grade, word_positions)
                    
                    matches.append(ConcreteMatch(
                        grade=self._normalize_concrete_grade(grade),
                        context=context[:200],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                        location=location,
                        confidence=0.8 if coordinates else 0.6,
                        method='ocr',
                        coordinates=coordinates
                    ))
        
        return matches

    def _find_word_coordinates(self, word: str, word_positions: Dict) -> Optional[Tuple[int, int, int, int]]:
        """–ù–∞—Ö–æ–¥–∏—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Å–ª–æ–≤–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"""
        for pos_word, coords in word_positions.items():
            if word.lower() in pos_word.lower() or pos_word.lower() in word.lower():
                return (coords['x'], coords['y'], coords['w'], coords['h'])
        return None

    def _identify_structural_element(self, context: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        context_upper = context.upper()
        
        # –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        for element, info in self.structural_elements.items():
            if element in context_upper:
                return f"{element} ({info['en']})"
        
        # –ü–æ–∏—Å–∫ —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        for element, info in self.structural_elements.items():
            if any(part in context_upper for part in element.split()):
                return f"{element} ({info['en']}) - —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ"
        
        return "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ"

    def _normalize_concrete_grade(self, grade: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ –º–∞—Ä–∫–∏ –±–µ—Ç–æ–Ω–∞"""
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
        normalized = re.sub(r'\s+', '', grade.upper())
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏
        normalized = re.sub(r'^B(\d+)$', r'C\1/\1', normalized)  # B20 -> C20/25
        normalized = re.sub(r'^(\d+)$', r'C\1', normalized)       # 30 -> C30
        
        return normalized

    # ==============================
    # üìÑ –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ PDF
    # ==============================
    def _parse_pdf_advanced(self, file_path: str) -> Tuple[str, List[ConcreteMatch]]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ PDF —Å —Ç–µ–∫—Å—Ç–æ–º –∏ OCR"""
        text_content = ""
        ocr_matches = []
        
        try:
            with pdfplumber.open(file_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                    page_text = page.extract_text() or ""
                    text_content += f"\n--- –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num + 1} ---\n{page_text}"
                    
                    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –º–∞–ª–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º OCR
                    if len(page_text.strip()) < 100:
                        try:
                            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                            img = page.to_image(resolution=300).original
                            
                            # OCR –∞–Ω–∞–ª–∏–∑
                            ocr_text, word_positions = self._extract_text_with_ocr(img)
                            
                            if ocr_text.strip():
                                text_content += f"\n--- OCR –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num + 1} ---\n{ocr_text}"
                                
                                # –ò—â–µ–º –º–∞—Ä–∫–∏ –±–µ—Ç–æ–Ω–∞ –Ω–∞ —á–µ—Ä—Ç–µ–∂–µ
                                drawing_matches = self._find_concrete_in_drawing(ocr_text, word_positions)
                                ocr_matches.extend(drawing_matches)
                        
                        except Exception as e:
                            logger.warning(f"OCR –æ—à–∏–±–∫–∞ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num + 1}: {e}")
            
            logger.info(f"üìÑ PDF –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {len(text_content)} —Å–∏–º–≤–æ–ª–æ–≤, {len(ocr_matches)} OCR —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF {file_path}: {e}")
        
        return text_content, ocr_matches

    # ==============================
    # üìä –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ XML —Å–º–µ—Ç
    # ==============================
    def _parse_xml_smeta_advanced(self, smeta_path: str) -> List[Dict]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–∞—Ä—Å–∏–Ω–≥ XML —Å–º–µ—Ç —Å –∫–æ–¥–∞–º–∏ –∏ –º–∞–ø–ø–∏–Ω–≥–æ–º"""
        items = []
        try:
            tree = ET.parse(smeta_path)
            root = tree.getroot()
            
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å–º–µ—Ç
            for elem in root.iter():
                item_data = {}
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–ª—è
                if elem.tag.lower() in ['item', 'row', 'position', 'pozice']:
                    # –ö–æ–¥ –ø–æ–∑–∏—Ü–∏–∏
                    code = elem.findtext('.//code') or elem.findtext('.//kod') or elem.get('code', '')
                    if code:
                        item_data['code'] = code.strip()
                        item_data['code_description'] = self.smeta_codes.get(code[:3], 'Nezn√°m√° kategorie')
                    
                    # –û–ø–∏—Å–∞–Ω–∏–µ
                    desc = elem.findtext('.//description') or elem.findtext('.//popis') or elem.findtext('.//name')
                    if desc and any(word in desc.lower() for word in ['beton', 'concrete', '≈æelezobeton']):
                        item_data['description'] = desc.strip()
                        
                        # –ò—â–µ–º –º–∞—Ä–∫–∏ –±–µ—Ç–æ–Ω–∞ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
                        concrete_grades = []
                        for pattern in self.concrete_patterns:
                            matches = re.findall(pattern, desc, re.IGNORECASE)
                            concrete_grades.extend([self._normalize_concrete_grade(m) for m in matches])
                        
                        if concrete_grades:
                            item_data['concrete_grades'] = list(set(concrete_grades))
                    
                    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏ –µ–¥–∏–Ω–∏—Ü–∞
                    qty = elem.findtext('.//quantity') or elem.findtext('.//mnozstvi')
                    if qty:
                        item_data['quantity'] = float(qty.replace(',', '.')) if qty.replace(',', '.').replace('.', '').isdigit() else qty
                    
                    unit = elem.findtext('.//unit') or elem.findtext('.//jednotka')
                    if unit:
                        item_data['unit'] = unit.strip()
                    
                    # –¶–µ–Ω–∞
                    price = elem.findtext('.//price') or elem.findtext('.//cena')
                    if price:
                        try:
                            item_data['price'] = float(price.replace(',', '.').replace(' ', ''))
                        except:
                            item_data['price'] = price
                    
                    if item_data:
                        items.append(item_data)
                
                # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä—è–º–æ–π —Ç–µ–∫—Å—Ç —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                elif elem.text and any(word in elem.text.lower() for word in ['beton', 'concrete']):
                    simple_item = {'description': elem.text.strip()}
                    
                    # –ò—â–µ–º –º–∞—Ä–∫–∏ –±–µ—Ç–æ–Ω–∞
                    concrete_grades = []
                    for pattern in self.concrete_patterns:
                        matches = re.findall(pattern, elem.text, re.IGNORECASE)
                        concrete_grades.extend([self._normalize_concrete_grade(m) for m in matches])
                    
                    if concrete_grades:
                        simple_item['concrete_grades'] = list(set(concrete_grades))
                    
                    items.append(simple_item)
            
            logger.info(f"üìä XML —Å–º–µ—Ç–∞: –Ω–∞–π–¥–µ–Ω–æ {len(items)} –ø–æ–∑–∏—Ü–∏–π")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML —Å–º–µ—Ç—ã: {e}")
        
        return items

    # ==============================
    # üß† –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    # ==============================
    def _local_concrete_analysis(self, text: str, ocr_matches: List[ConcreteMatch] = None) -> Dict[str, Any]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤"""
        all_matches = []
        
        # 1. –û–±—ã—á–Ω—ã–π regex –ø–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ
        for pattern in self.concrete_patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                grade = self._normalize_concrete_grade(match.group().strip())
                context = text[max(0, match.start()-100):match.end()+100]
                location = self._identify_structural_element(context)
                
                all_matches.append(ConcreteMatch(
                    grade=grade,
                    context=context.strip(),
                    location=location,
                    confidence=0.9,
                    method='regex'
                ))
        
        # 2. –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã OCR
        if ocr_matches:
            all_matches.extend(ocr_matches)
        
        # 3. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞
        unique_matches = {}
        for match in all_matches:
            key = match.grade
            if key not in unique_matches or match.confidence > unique_matches[key].confidence:
                unique_matches[key] = match
        
        # 4. –û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ knowledge base
        enriched_results = []
        for match in unique_matches.values():
            kb_info = self._lookup_in_kb(match.grade, match.context)
            
            result = {
                'grade': match.grade,
                'location': match.location,
                'context': match.context[:200],
                'confidence': match.confidence,
                'method': match.method,
                'kb_info': kb_info,
                'coordinates': match.coordinates
            }
            enriched_results.append(result)
        
        return {
            'concrete_summary': enriched_results,
            'analysis_method': 'enhanced_local',
            'total_matches': len(enriched_results),
            'success': True
        }

    def _lookup_in_kb(self, grade: str, context: str) -> Dict[str, Any]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
        kb_classes = self.knowledge_base.get('concrete_knowledge_base', {}).get('environment_classes', {})
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª–∞—Å—Å—ã –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è –∏–∑ –º–∞—Ä–∫–∏
        exposure_classes = re.findall(r'X[CDFASM]\d*', grade, re.IGNORECASE)
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–∞—Ö
        class_info = {}
        applications = []
        
        for exp_class in exposure_classes:
            exp_class_upper = exp_class.upper()
            if exp_class_upper in kb_classes:
                class_info[exp_class_upper] = kb_classes[exp_class_upper]
                applications.extend(kb_classes[exp_class_upper].get('applications', []))
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context_applications = []
        context_upper = context.upper()
        
        for element, info in self.structural_elements.items():
            if element in context_upper:
                context_applications.extend(info['applications'])
        
        return {
            'exposure_classes': exposure_classes,
            'class_details': class_info,
            'recommended_applications': list(set(applications)),
            'context_applications': list(set(context_applications)),
            'compliance': len(set(applications).intersection(set(context_applications))) > 0
        }

    # ==============================
    # ü§ñ Claude –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
    # ==============================
    async def _claude_concrete_analysis(self, text: str, smeta_data: List[Dict]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Claude —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        if not self.claude_client:
            return {"success": False, "error": "Claude client not available"}
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏
            context_prompt = f"""
–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:

–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ:
{', '.join(self.structural_elements.keys())}

–ö–æ–¥—ã —Å–º–µ—Ç, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ:
{[item.get('code', 'N/A') for item in smeta_data if 'code' in item]}

–¢—Ä–µ–±—É–µ—Ç—Å—è –Ω–∞–π—Ç–∏ –∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å:
1. –í—Å–µ –º–∞—Ä–∫–∏ –±–µ—Ç–æ–Ω–∞ (–≤–∫–ª—é—á–∞—è C30/37-XF2,XD1,XC4)
2. –ö–ª–∞—Å—Å—ã –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è —Å—Ä–µ–¥—ã (XC, XD, XF, XA, XM)
3. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–∞—Ä–æ–∫ –º–µ—Å—Ç–∞–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —á–µ—à—Å–∫–∏–º –Ω–æ—Ä–º–∞–º ƒåSN EN 206
            """
            
            result = await self.claude_client.analyze_concrete_with_claude(
                text + "\n\n" + context_prompt, 
                smeta_data
            )
            
            return {
                'concrete_summary': result.get('claude_analysis', {}).get('concrete_grades', []),
                'analysis_method': 'claude_enhanced',
                'success': True,
                'tokens_used': result.get('tokens_used', 0),
                'raw_response': result.get('raw_response', '')
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Claude –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return {"success": False, "error": str(e)}

    # ==============================
    # üéØ –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞
    # ==============================
    async def analyze(self, doc_paths: List[str], smeta_path: Optional[str] = None,
                      use_claude: bool = True, claude_mode: str = "enhancement") -> Dict[str, Any]:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
        """
        logger.info(f"üèóÔ∏è –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (—Ä–µ–∂–∏–º: {claude_mode})")
        
        all_text = ""
        all_ocr_matches = []
        processed_docs = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        for doc_path in doc_paths:
            try:
                if doc_path.lower().endswith('.pdf'):
                    text, ocr_matches = self._parse_pdf_advanced(doc_path)
                    all_text += text
                    all_ocr_matches.extend(ocr_matches)
                else:
                    text = self.doc_parser.parse(doc_path)
                    all_text += text
                
                processed_docs.append({
                    'file': Path(doc_path).name,
                    'type': 'PDF' if doc_path.lower().endswith('.pdf') else 'Document',
                    'text_length': len(text),
                    'ocr_matches': len(ocr_matches) if doc_path.lower().endswith('.pdf') else 0
                })
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {doc_path}: {e}")
                processed_docs.append({'file': Path(doc_path).name, 'error': str(e)})
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–º–µ—Ç—É
        smeta_data = []
        if smeta_path:
            try:
                if smeta_path.lower().endswith('.xml'):
                    smeta_data = self._parse_xml_smeta_advanced(smeta_path)
                else:
                    smeta_result = self.smeta_parser.parse(smeta_path)
                    smeta_data = smeta_result.get('items', [])
                
                logger.info(f"üìä –°–º–µ—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞: {len(smeta_data)} –ø–æ–∑–∏—Ü–∏–π")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–º–µ—Ç—ã: {e}")
        
        # –õ–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å OCR
        local_result = self._local_concrete_analysis(all_text, all_ocr_matches)
        
        # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Claude
        final_result = local_result.copy()
        
        if use_claude and self.claude_client:
            claude_result = await self._claude_concrete_analysis(all_text, smeta_data)
            
            if claude_mode == "primary" and claude_result.get("success"):
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ Claude —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                final_result = claude_result
            elif claude_mode == "enhancement" and claude_result.get("success"):
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –∏ Claude —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                final_result.update({
                    'claude_analysis': claude_result,
                    'analysis_method': 'hybrid_enhanced',
                    'total_tokens_used': claude_result.get('tokens_used', 0)
                })
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        final_result.update({
            'processed_documents': processed_docs,
            'smeta_items': len(smeta_data),
            'processing_time': 'completed',
            'knowledge_base_version': '3.0',
            'ocr_enabled': True,
            'features_used': [
                'regex_patterns',
                'ocr_analysis', 
                'knowledge_base_lookup',
                'structural_element_detection',
                'xml_smeta_parsing'
            ] + (['claude_enhancement'] if use_claude else [])
        })
        
        return final_result


# ==============================
