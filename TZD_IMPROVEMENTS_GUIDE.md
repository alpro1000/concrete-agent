# üõ†Ô∏è TZDReader: –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –£–ª—É—á—à–µ–Ω–∏—è –¥–ª—è –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏

**–°–æ–ø—É—Ç—Å—Ç–≤—É—é—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –∫:** TZD_ARCHITECTURE_REVIEW.md  
**–§–æ–∫—É—Å:** –ì–æ—Ç–æ–≤—ã–µ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —Ä–µ—à–µ–Ω–∏—è –∏ –∫–æ–¥

---

## üéØ –ö—Ä–∞—Ç–∫–æ–µ –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –í–Ω–µ–¥—Ä–µ–Ω–∏—é

–≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ TZDReader –∞–≥–µ–Ω—Ç–∞. –ö–∞–∂–¥–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≤–∫–ª—é—á–∞–µ—Ç:
- ‚úÖ –ì–æ—Ç–æ–≤—ã–π –∫–æ–¥
- üìù –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
- üß™ –ü—Ä–∏–º–µ—Ä—ã —Ç–µ—Å—Ç–æ–≤
- üìä –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

---

## 1Ô∏è‚É£ –£–ª—É—á—à–µ–Ω–Ω–∞—è –û–±—Ä–∞–±–æ—Ç–∫–∞ –û—à–∏–±–æ–∫

### –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ `agents/tzd_reader/exceptions.py`

```python
"""
–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –¥–ª—è TZDReader
–û–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç —Ç–æ—á–Ω—É—é –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫
"""
from typing import Dict, Any, Optional


class TZDReaderError(Exception):
    """–ë–∞–∑–æ–≤–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –≤—Å–µ—Ö –æ—à–∏–±–æ–∫ TZDReader"""
    
    def __init__(
        self, 
        message: str, 
        details: Optional[Dict[str, Any]] = None,
        recoverable: bool = False
    ):
        self.message = message
        self.details = details or {}
        self.recoverable = recoverable
        super().__init__(self.message)
    
    def to_dict(self) -> Dict[str, Any]:
        """–°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è API –æ—Ç–≤–µ—Ç–æ–≤"""
        return {
            'error_type': self.__class__.__name__,
            'message': self.message,
            'details': self.details,
            'recoverable': self.recoverable
        }


class ValidationError(TZDReaderError):
    """–û—à–∏–±–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    def __init__(self, message: str, field: str = None, **kwargs):
        details = {'field': field} if field else {}
        super().__init__(message, details=details, recoverable=True, **kwargs)


class EmptyFileListError(ValidationError):
    """–ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤"""
    def __init__(self):
        super().__init__("File list cannot be empty", field="files")


class TooManyFilesError(ValidationError):
    """–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ñ–∞–π–ª–æ–≤"""
    def __init__(self, count: int, max_count: int = 20):
        super().__init__(
            f"Too many files: {count} (max: {max_count})",
            field="files",
            details={'count': count, 'max_count': max_count}
        )


class ParsingError(TZDReaderError):
    """–û—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    def __init__(self, message: str, file_path: str = None, **kwargs):
        details = {'file_path': file_path} if file_path else {}
        super().__init__(message, details=details, recoverable=False, **kwargs)


class EmptyContentError(ParsingError):
    """–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
    def __init__(self, file_path: str):
        super().__init__(
            f"Could not extract text from file",
            file_path=file_path
        )


class AIServiceError(TZDReaderError):
    """–û—à–∏–±–∫–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å AI —Å–µ—Ä–≤–∏—Å–æ–º"""
    def __init__(
        self, 
        message: str, 
        provider: str = None,
        model: str = None,
        **kwargs
    ):
        details = {}
        if provider:
            details['provider'] = provider
        if model:
            details['model'] = model
        super().__init__(message, details=details, recoverable=False, **kwargs)


class TemporaryError(TZDReaderError):
    """–í—Ä–µ–º–µ–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏, —Ç—Ä–µ–±—É—é—â–∏–µ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏"""
    def __init__(self, message: str, retry_after: int = None, **kwargs):
        details = {'retry_after': retry_after} if retry_after else {}
        super().__init__(message, details=details, recoverable=True, **kwargs)


class RateLimitError(TemporaryError):
    """–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤"""
    def __init__(self, provider: str, retry_after: int = 60):
        super().__init__(
            f"Rate limit exceeded for {provider}",
            retry_after=retry_after,
            details={'provider': provider}
        )


class TimeoutError(TemporaryError):
    """–¢–∞–π–º–∞—É—Ç –æ–ø–µ—Ä–∞—Ü–∏–∏"""
    def __init__(self, operation: str, timeout: int):
        super().__init__(
            f"Operation '{operation}' timed out after {timeout}s",
            details={'operation': operation, 'timeout': timeout}
        )


class ConfigurationError(TZDReaderError):
    """–û—à–∏–±–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    def __init__(self, message: str, config_key: str = None, **kwargs):
        details = {'config_key': config_key} if config_key else {}
        super().__init__(message, details=details, recoverable=False, **kwargs)
```

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ `agent.py`

```python
# –í –Ω–∞—á–∞–ª–µ —Ñ–∞–π–ª–∞
from .exceptions import (
    TZDReaderError,
    ValidationError,
    EmptyFileListError,
    TooManyFilesError,
    ParsingError,
    EmptyContentError,
    AIServiceError,
    TemporaryError,
    RateLimitError,
    TimeoutError,
    ConfigurationError
)

# –û–±–Ω–æ–≤–∏—Ç—å tzd_reader —Ñ—É–Ω–∫—Ü–∏—é
def tzd_reader(
    files: List[str], 
    engine: str = "gpt", 
    base_dir: Optional[str] = None
) -> Dict[str, Any]:
    """Main function with improved error handling"""
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    if not files:
        raise EmptyFileListError()
    
    if len(files) > 20:
        raise TooManyFilesError(count=len(files))
    
    if engine.lower() not in ['gpt', 'claude', 'auto']:
        raise ValidationError(
            f"Unsupported AI engine: {engine}",
            field="engine",
            details={'supported': ['gpt', 'claude', 'auto']}
        )
    
    start_time = time.time()
    
    try:
        combined_text = _process_files_with_parsers(files, base_dir)
        
        if not combined_text.strip():
            raise EmptyContentError("Could not extract text from any file")
        
        analyzer = SecureAIAnalyzer()
        
        # ... –æ—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ ...
        
    except (ValidationError, SecurityError, ParsingError) as e:
        # –û–∂–∏–¥–∞–µ–º—ã–µ –æ—à–∏–±–∫–∏ - –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –¥–∞–ª—å—à–µ
        logger.error(f"Expected error in tzd_reader: {e}", exc_info=True)
        raise
        
    except TemporaryError as e:
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏ - –ª–æ–≥–∏—Ä—É–µ–º –∏ –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º
        logger.warning(f"Temporary error in tzd_reader: {e}")
        raise
        
    except Exception as e:
        # –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏ - –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ TZDReaderError
        logger.error(f"Unexpected error in tzd_reader: {e}", exc_info=True)
        raise TZDReaderError(
            f"Unexpected error during analysis: {str(e)}",
            details={'error_type': type(e).__name__}
        ) from e
```

### –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ —Ä–æ—É—Ç–µ—Ä–µ

```python
# app/routers/tzd_router.py
from agents.tzd_reader.exceptions import (
    TZDReaderError,
    ValidationError,
    TemporaryError
)

@router.post("/analyze")
async def analyze_tzd_documents(...):
    try:
        result = tzd_reader(files=file_paths, engine=ai_engine)
        # ... success handling ...
        
    except ValidationError as e:
        # –û—à–∏–±–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ - 400 Bad Request
        raise HTTPException(
            status_code=400,
            detail=e.to_dict()
        )
        
    except TemporaryError as e:
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏ - 503 Service Unavailable
        retry_after = e.details.get('retry_after', 60)
        raise HTTPException(
            status_code=503,
            detail=e.to_dict(),
            headers={'Retry-After': str(retry_after)}
        )
        
    except TZDReaderError as e:
        # –î—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏ TZDReader - 500 Internal Server Error
        logger.error(f"TZD analysis error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=e.to_dict()
        )
        
    except Exception as e:
        # –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏
        logger.error(f"Unexpected error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail={
                'error_type': 'UnexpectedError',
                'message': 'An unexpected error occurred',
                'recoverable': False
            }
        )
```

---

## 2Ô∏è‚É£ Retry-–ª–æ–≥–∏–∫–∞ —Å Exponential Backoff

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

```bash
pip install tenacity
```

### –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ `agent.py`

```python
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log
)

class SecureAIAnalyzer:
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type(TemporaryError),
        before_sleep=before_sleep_log(logger, logging.WARNING)
    )
    async def analyze_with_llm_service_with_retry(
        self, 
        text: str, 
        provider: str = "claude"
    ) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–æ–≤—Ç–æ—Ä–∞–º–∏ –ø—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–±–æ—è—Ö"""
        
        try:
            response = await self.llm_service.run_prompt(
                provider=provider,
                prompt=text,
                system_prompt=self.get_analysis_prompt(),
                model=self._get_model(provider),
                max_tokens=4000
            )
            
            if not response.get("success"):
                error_msg = response.get("error", "Unknown error")
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ—à–∏–±–∫–∏
                if self._is_rate_limit_error(error_msg):
                    raise RateLimitError(provider=provider, retry_after=60)
                elif self._is_timeout_error(error_msg):
                    raise TimeoutError(operation="llm_analysis", timeout=60)
                elif self._is_temporary_error(error_msg):
                    raise TemporaryError(f"Temporary error: {error_msg}")
                else:
                    raise AIServiceError(
                        f"AI service error: {error_msg}",
                        provider=provider
                    )
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —É—Å–ø–µ—à–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
            result_text = response.get("content", "")
            if not result_text:
                raise AIServiceError("Empty response from LLM service")
            
            json_text = self._extract_json_from_response(result_text)
            result = json.loads(json_text)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            result['ai_model'] = response.get("model")
            result['provider'] = provider
            result['retry_count'] = getattr(self, '_retry_count', 0)
            
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON from LLM: {e}")
            raise AIServiceError(
                f"Invalid JSON response: {e}",
                provider=provider
            )
    
    def _is_rate_limit_error(self, error_msg: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ rate limit –æ—à–∏–±–∫—É"""
        rate_limit_keywords = [
            'rate limit',
            'too many requests',
            'quota exceeded',
            '429'
        ]
        return any(kw in error_msg.lower() for kw in rate_limit_keywords)
    
    def _is_timeout_error(self, error_msg: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ timeout –æ—à–∏–±–∫—É"""
        timeout_keywords = ['timeout', 'timed out', 'deadline exceeded']
        return any(kw in error_msg.lower() for kw in timeout_keywords)
    
    def _is_temporary_error(self, error_msg: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω—É—é –æ—à–∏–±–∫—É"""
        temporary_keywords = [
            'temporary',
            'service unavailable',
            'connection error',
            '503',
            '502'
        ]
        return any(kw in error_msg.lower() for kw in temporary_keywords)
    
    def _get_model(self, provider: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        if provider == "claude":
            return self.claude_model
        elif provider == "openai":
            return self.openai_model
        else:
            return "auto"
```

### –¢–µ—Å—Ç—ã –¥–ª—è retry-–ª–æ–≥–∏–∫–∏

```python
# tests/test_retry_logic.py
import pytest
from unittest.mock import AsyncMock, patch
from agents.tzd_reader.agent import SecureAIAnalyzer
from agents.tzd_reader.exceptions import RateLimitError, TemporaryError

@pytest.mark.asyncio
async def test_retry_on_rate_limit():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ rate limit"""
    analyzer = SecureAIAnalyzer()
    
    # –ú–æ–∫–∏—Ä—É–µ–º LLM service
    with patch.object(analyzer, 'llm_service') as mock_llm:
        # –ü–µ—Ä–≤—ã–µ 2 –ø–æ–ø—ã—Ç–∫–∏ - rate limit, —Ç—Ä–µ—Ç—å—è - —É—Å–ø–µ—Ö
        mock_llm.run_prompt = AsyncMock(side_effect=[
            {'success': False, 'error': 'Rate limit exceeded'},
            {'success': False, 'error': 'Rate limit exceeded'},
            {
                'success': True,
                'content': '{"project_object": "Test"}',
                'model': 'gpt-4'
            }
        ])
        
        result = await analyzer.analyze_with_llm_service_with_retry(
            text="Test text",
            provider="openai"
        )
        
        assert result['project_object'] == "Test"
        assert mock_llm.run_prompt.call_count == 3

@pytest.mark.asyncio
async def test_retry_exhausted():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—á–µ—Ä–ø–∞–Ω–∏—è –ø–æ–ø—ã—Ç–æ–∫"""
    analyzer = SecureAIAnalyzer()
    
    with patch.object(analyzer, 'llm_service') as mock_llm:
        # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ - rate limit
        mock_llm.run_prompt = AsyncMock(return_value={
            'success': False,
            'error': 'Rate limit exceeded'
        })
        
        with pytest.raises(RateLimitError):
            await analyzer.analyze_with_llm_service_with_retry(
                text="Test text",
                provider="openai"
            )
        
        assert mock_llm.run_prompt.call_count == 3
```

---

## 3Ô∏è‚É£ –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
pip install structlog python-json-logger
```

### –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ `utils/logging_config.py`

```python
"""
–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è TZDReader
"""
import logging
import logging.config
import sys
from pathlib import Path
from typing import Any

import structlog
from pythonjsonlogger import jsonlogger


def setup_structured_logging(log_level: str = "INFO"):
    """
    –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    
    Args:
        log_level: –£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (DEBUG, INFO, WARNING, ERROR)
    """
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ª–æ–≥–æ–≤
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ logging
    logging.config.dictConfig({
        "version": 1,
        "disable_existing_loggers": False,
        "formatters": {
            "json": {
                "()": jsonlogger.JsonFormatter,
                "format": "%(asctime)s %(name)s %(levelname)s %(message)s"
            },
            "console": {
                "()": structlog.stdlib.ProcessorFormatter,
                "processor": structlog.dev.ConsoleRenderer(colors=True),
            },
        },
        "handlers": {
            "console": {
                "class": "logging.StreamHandler",
                "formatter": "console",
                "stream": sys.stdout,
            },
            "file": {
                "class": "logging.handlers.RotatingFileHandler",
                "filename": "logs/tzd_reader.log",
                "maxBytes": 10485760,  # 10MB
                "backupCount": 5,
                "formatter": "json",
            },
            "error_file": {
                "class": "logging.handlers.RotatingFileHandler",
                "filename": "logs/tzd_reader_errors.log",
                "maxBytes": 10485760,
                "backupCount": 5,
                "formatter": "json",
                "level": "ERROR",
            },
        },
        "root": {
            "handlers": ["console", "file", "error_file"],
            "level": log_level,
        },
        "loggers": {
            "agents.tzd_reader": {
                "handlers": ["console", "file", "error_file"],
                "level": log_level,
                "propagate": False,
            },
        },
    })
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ structlog
    structlog.configure(
        processors=[
            structlog.contextvars.merge_contextvars,
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        cache_logger_on_first_use=True,
    )


def get_logger(name: str) -> structlog.BoundLogger:
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ª–æ–≥–≥–µ—Ä
    
    Args:
        name: –ò–º—è –º–æ–¥—É–ª—è
        
    Returns:
        –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π structlog –ª–æ–≥–≥–µ—Ä
    """
    return structlog.get_logger(name)


# Utility —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
class LogContext:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self, **kwargs):
        self.context = kwargs
    
    def __enter__(self):
        structlog.contextvars.clear_contextvars()
        structlog.contextvars.bind_contextvars(**self.context)
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        structlog.contextvars.clear_contextvars()
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ `agent.py`

```python
from utils.logging_config import get_logger, LogContext
import uuid

logger = get_logger(__name__)

def tzd_reader(
    files: List[str], 
    engine: str = "gpt", 
    base_dir: Optional[str] = None
) -> Dict[str, Any]:
    """Main function with structured logging"""
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –∑–∞–ø—Ä–æ—Å–∞
    request_id = str(uuid.uuid4())
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    with LogContext(
        request_id=request_id,
        component="tzd_reader",
        files_count=len(files),
        engine=engine
    ):
        logger.info(
            "analysis_started",
            files=[Path(f).name for f in files]
        )
        
        start_time = time.time()
        
        try:
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            if not files:
                logger.error("validation_failed", reason="empty_file_list")
                raise EmptyFileListError()
            
            logger.debug("validation_passed")
            
            # –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–æ–≤
            logger.info("parsing_started")
            combined_text = _process_files_with_parsers(files, base_dir)
            logger.info(
                "parsing_completed",
                text_length=len(combined_text)
            )
            
            # AI –∞–Ω–∞–ª–∏–∑
            logger.info("ai_analysis_started", provider=engine)
            analyzer = SecureAIAnalyzer()
            result = analyzer.analyze_with_gpt(combined_text)
            logger.info(
                "ai_analysis_completed",
                ai_model=result.get('ai_model')
            )
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            duration = time.time() - start_time
            result['processing_metadata'] = {
                'request_id': request_id,
                'processed_files': len(files),
                'processing_time_seconds': round(duration, 2),
                'ai_engine': engine.lower(),
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            logger.info(
                "analysis_completed_successfully",
                duration=duration,
                processed_files=len(files)
            )
            
            return result
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error(
                "analysis_failed",
                error=str(e),
                error_type=type(e).__name__,
                duration=duration,
                exc_info=True
            )
            raise
```

---

## 4Ô∏è‚É£ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ú–µ—Ç—Ä–∏–∫–∏

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Prometheus client

```bash
pip install prometheus-client
```

### –°–æ–∑–¥–∞–Ω–∏–µ `agents/tzd_reader/monitoring.py`

```python
"""
–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è TZDReader
"""
from prometheus_client import Counter, Histogram, Gauge, Info
from functools import wraps
import time
from typing import Callable, Any
from contextlib import contextmanager

# –°—á–µ—Ç—á–∏–∫–∏
tzd_requests_total = Counter(
    'tzd_requests_total',
    'Total number of TZD analysis requests',
    ['engine', 'status']
)

tzd_files_processed_total = Counter(
    'tzd_files_processed_total',
    'Total number of files processed',
    ['file_type', 'status']
)

tzd_errors_total = Counter(
    'tzd_errors_total',
    'Total number of errors',
    ['error_type', 'recoverable']
)

# –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã (–¥–ª—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏)
tzd_processing_duration_seconds = Histogram(
    'tzd_processing_duration_seconds',
    'Time spent processing TZD requests',
    ['engine', 'operation'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0]
)

tzd_file_size_bytes = Histogram(
    'tzd_file_size_bytes',
    'Size of processed files in bytes',
    ['file_type'],
    buckets=[1024, 10240, 102400, 1048576, 10485760]  # 1KB to 10MB
)

# Gauge (—Ç–µ–∫—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è)
tzd_active_requests = Gauge(
    'tzd_active_requests',
    'Number of active TZD requests'
)

# Info (—Å—Ç–∞—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)
tzd_info = Info(
    'tzd_info',
    'Information about TZD Reader service'
)

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
tzd_info.info({
    'version': '2.0',
    'supported_engines': 'gpt,claude,auto',
    'max_file_size_mb': '10',
    'supported_formats': 'pdf,docx,txt'
})


class MetricsCollector:
    """–°–±–æ—Ä—â–∏–∫ –º–µ—Ç—Ä–∏–∫ –¥–ª—è TZDReader"""
    
    @staticmethod
    @contextmanager
    def track_request(engine: str):
        """
        –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞
        
        Usage:
            with MetricsCollector.track_request("gpt"):
                result = tzd_reader(files, "gpt")
        """
        tzd_active_requests.inc()
        start_time = time.time()
        status = "error"
        
        try:
            yield
            status = "success"
        finally:
            duration = time.time() - start_time
            tzd_processing_duration_seconds.labels(
                engine=engine,
                operation="full_analysis"
            ).observe(duration)
            tzd_requests_total.labels(engine=engine, status=status).inc()
            tzd_active_requests.dec()
    
    @staticmethod
    def track_file(file_path: str, file_size: int, status: str = "success"):
        """–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞"""
        from pathlib import Path
        file_type = Path(file_path).suffix.lstrip('.')
        
        tzd_files_processed_total.labels(
            file_type=file_type,
            status=status
        ).inc()
        
        tzd_file_size_bytes.labels(file_type=file_type).observe(file_size)
    
    @staticmethod
    def track_error(error: Exception, recoverable: bool = False):
        """–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –æ—à–∏–±–∫–∏"""
        error_type = type(error).__name__
        tzd_errors_total.labels(
            error_type=error_type,
            recoverable=str(recoverable).lower()
        ).inc()
    
    @staticmethod
    def track_operation(operation: str):
        """
        –î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–π
        
        Usage:
            @MetricsCollector.track_operation("parsing")
            def parse_file(file_path):
                ...
        """
        def decorator(func: Callable) -> Callable:
            @wraps(func)
            def wrapper(*args, **kwargs) -> Any:
                start_time = time.time()
                engine = kwargs.get('engine', 'unknown')
                
                try:
                    result = func(*args, **kwargs)
                    return result
                finally:
                    duration = time.time() - start_time
                    tzd_processing_duration_seconds.labels(
                        engine=engine,
                        operation=operation
                    ).observe(duration)
            
            return wrapper
        return decorator


# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
def get_metrics_summary() -> dict:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –º–µ—Ç—Ä–∏–∫ –¥–ª—è health check"""
    from prometheus_client import REGISTRY
    
    metrics = {}
    for collector in REGISTRY._collector_to_names:
        if hasattr(collector, '_name') and collector._name.startswith('tzd_'):
            metrics[collector._name] = collector._value.get()
    
    return metrics
```

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –≤ `agent.py`

```python
from .monitoring import MetricsCollector

def tzd_reader(
    files: List[str], 
    engine: str = "gpt", 
    base_dir: Optional[str] = None
) -> Dict[str, Any]:
    """Main function with metrics"""
    
    with MetricsCollector.track_request(engine):
        try:
            # ... –æ—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ ...
            
            # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
            for file_path in files:
                if os.path.exists(file_path):
                    file_size = os.path.getsize(file_path)
                    MetricsCollector.track_file(
                        file_path, 
                        file_size, 
                        status="success"
                    )
            
            return result
            
        except Exception as e:
            # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –æ—à–∏–±–∫—É
            recoverable = isinstance(e, (ValidationError, TemporaryError))
            MetricsCollector.track_error(e, recoverable=recoverable)
            raise

# –î–µ–∫–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
@MetricsCollector.track_operation("parsing")
def _process_files_with_parsers(files: List[str], base_dir: Optional[str] = None) -> str:
    """Process files with metrics tracking"""
    # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...
```

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞

```python
# app/main.py
from prometheus_client import make_asgi_app

# –°–æ–∑–¥–∞–µ–º ASGI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –º–µ—Ç—Ä–∏–∫
metrics_app = make_asgi_app()

# –ú–æ–Ω—Ç–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
app.mount("/metrics", metrics_app)
```

### Health check —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏

```python
# app/routers/tzd_router.py
from agents.tzd_reader.monitoring import get_metrics_summary

@router.get("/health/metrics")
async def health_with_metrics():
    """Health check —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    return {
        "service": "TZD Reader",
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "metrics": get_metrics_summary()
    }
```

---

## 5Ô∏è‚É£ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –û–±—Ä–∞–±–æ—Ç–∫–∞ –§–∞–π–ª–æ–≤

### –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ `agent.py` –¥–ª—è async

```python
import asyncio
from concurrent.futures import ProcessPoolExecutor
from typing import List, Dict, Any, Optional

async def _process_single_file_async(
    file_path: str,
    base_dir: Optional[str],
    validator: FileSecurityValidator,
    doc_parser
) -> Optional[Dict[str, str]]:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
    
    try:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è (–±—ã—Å—Ç—Ä–∞—è, –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø–æ—Ç–æ–∫–µ)
        validator.validate_all(file_path, base_dir)
        
        # –ü–∞—Ä—Å–∏–Ω–≥ (–º–µ–¥–ª–µ–Ω–Ω—ã–π, –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ)
        loop = asyncio.get_event_loop()
        with ProcessPoolExecutor() as executor:
            text = await loop.run_in_executor(
                executor,
                doc_parser.parse,
                file_path
            )
        
        if text and text.strip():
            normalized_text = normalize_diacritics(text)
            return {
                'file_path': file_path,
                'text': normalized_text,
                'size': len(normalized_text)
            }
        
        logger.warning(f"Empty text in file: {file_path}")
        return None
        
    except SecurityError as e:
        logger.error(f"Security validation failed for {file_path}: {e}")
        raise
        
    except Exception as e:
        logger.error(f"File processing error {file_path}: {e}")
        return None


async def _process_files_async(
    files: List[str],
    base_dir: Optional[str] = None
) -> str:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤"""
    
    validator = FileSecurityValidator()
    doc_parser = DocParser()
    
    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
    tasks = [
        _process_single_file_async(file_path, base_dir, validator, doc_parser)
        for file_path in files
    ]
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ —Å–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    text_parts = []
    processed_count = 0
    
    for result in results:
        if isinstance(result, Exception):
            logger.error(f"Processing exception: {result}")
            continue
        
        if result is None:
            continue
        
        file_header = f"\n=== File: {Path(result['file_path']).name} ===\n"
        text_parts.extend([file_header, result['text'], "\n"])
        processed_count += 1
    
    if not text_parts:
        raise EmptyContentError("Could not extract text from any file")
    
    combined_text = ''.join(text_parts)
    logger.info(f"Processed {processed_count}/{len(files)} files, {len(combined_text)} chars")
    
    return combined_text


# –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def _process_files_with_parsers(
    files: List[str],
    base_dir: Optional[str] = None
) -> str:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –Ω–∞–¥ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
    return asyncio.run(_process_files_async(files, base_dir))
```

---

## 6Ô∏è‚É£ –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### –°–æ–∑–¥–∞–Ω–∏–µ `agents/tzd_reader/cache.py`

```python
"""
–ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ TZDReader –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
"""
import hashlib
import json
import time
from typing import Dict, Any, Optional, List
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class TZDCache:
    """
    –ö–µ—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ TZDReader
    
    Features:
    - –•–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
    - LRU eviction –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–∞
    - TTL –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏—Å—Ç–µ—á–µ–Ω–∏—è –∫–µ—à–∞
    """
    
    def __init__(
        self,
        max_size: int = 100,
        ttl_seconds: int = 3600  # 1 —á–∞—Å
    ):
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self._cache: Dict[str, Dict[str, Any]] = {}
        self._access_times: Dict[str, float] = {}
    
    def _compute_file_hash(self, file_path: str) -> str:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ SHA256 —Ö–µ—à–∞ —Ñ–∞–π–ª–∞"""
        hasher = hashlib.sha256()
        
        try:
            with open(file_path, 'rb') as f:
                # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª —á–∞—Å—Ç—è–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                for chunk in iter(lambda: f.read(8192), b""):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except Exception as e:
            logger.error(f"Error hashing file {file_path}: {e}")
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç—å –∏ –≤—Ä–µ–º—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
            stat = Path(file_path).stat()
            fallback = f"{file_path}:{stat.st_size}:{stat.st_mtime}"
            return hashlib.sha256(fallback.encode()).hexdigest()
    
    def _generate_cache_key(
        self,
        files: List[str],
        engine: str
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫–µ—à–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∞–π–ª–æ–≤ –∏ –¥–≤–∏–∂–∫–∞"""
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        sorted_files = sorted(files)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à–∏ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        file_hashes = [self._compute_file_hash(f) for f in sorted_files]
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ö–µ—à–∏ —Å –¥–≤–∏–∂–∫–æ–º
        combined = f"{','.join(file_hashes)}|{engine.lower()}"
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ö–µ—à
        return hashlib.sha256(combined.encode()).hexdigest()
    
    def get(
        self,
        files: List[str],
        engine: str
    ) -> Optional[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–∑ –∫–µ—à–∞
        
        Returns:
            Cached result or None if not found or expired
        """
        cache_key = self._generate_cache_key(files, engine)
        
        if cache_key not in self._cache:
            logger.debug(f"Cache miss for key: {cache_key[:16]}...")
            return None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º TTL
        cache_time = self._access_times[cache_key]
        age = time.time() - cache_time
        
        if age > self.ttl_seconds:
            logger.info(f"Cache expired for key: {cache_key[:16]}... (age: {age}s)")
            self._remove(cache_key)
            return None
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –¥–æ—Å—Ç—É–ø–∞ (–¥–ª—è LRU)
        self._access_times[cache_key] = time.time()
        
        logger.info(f"Cache hit for key: {cache_key[:16]}... (age: {age}s)")
        return self._cache[cache_key].copy()
    
    def set(
        self,
        files: List[str],
        engine: str,
        result: Dict[str, Any]
    ) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –∫–µ—à"""
        cache_key = self._generate_cache_key(files, engine)
        
        # –ï—Å–ª–∏ –∫–µ—à –ø–æ–ª–æ–Ω, —É–¥–∞–ª—è–µ–º —Å–∞–º—ã–π —Å—Ç–∞—Ä—ã–π —ç–ª–µ–º–µ–Ω—Ç (LRU)
        if len(self._cache) >= self.max_size and cache_key not in self._cache:
            self._evict_lru()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self._cache[cache_key] = result.copy()
        self._access_times[cache_key] = time.time()
        
        logger.info(
            f"Cached result for key: {cache_key[:16]}... "
            f"(cache size: {len(self._cache)}/{self.max_size})"
        )
    
    def _evict_lru(self) -> None:
        """–£–¥–∞–ª–µ–Ω–∏–µ –Ω–∞–∏–º–µ–Ω–µ–µ –Ω–µ–¥–∞–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        if not self._access_times:
            return
        
        # –ù–∞—Ö–æ–¥–∏–º –∫–ª—é—á —Å —Å–∞–º—ã–º —Å—Ç–∞—Ä—ã–º –≤—Ä–µ–º–µ–Ω–µ–º –¥–æ—Å—Ç—É–ø–∞
        oldest_key = min(self._access_times.items(), key=lambda x: x[1])[0]
        self._remove(oldest_key)
        logger.debug(f"Evicted LRU entry: {oldest_key[:16]}...")
    
    def _remove(self, cache_key: str) -> None:
        """–£–¥–∞–ª–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–∞ –∏–∑ –∫–µ—à–∞"""
        self._cache.pop(cache_key, None)
        self._access_times.pop(cache_key, None)
    
    def clear(self) -> None:
        """–û—á–∏—Å—Ç–∫–∞ –≤—Å–µ–≥–æ –∫–µ—à–∞"""
        count = len(self._cache)
        self._cache.clear()
        self._access_times.clear()
        logger.info(f"Cache cleared ({count} entries removed)")
    
    def get_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–µ—à–∞"""
        return {
            'size': len(self._cache),
            'max_size': self.max_size,
            'ttl_seconds': self.ttl_seconds,
            'usage_percent': (len(self._cache) / self.max_size * 100) if self.max_size > 0 else 0
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–µ—à–∞
_global_cache = TZDCache()


def get_cache() -> TZDCache:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –∫–µ—à–∞"""
    return _global_cache
```

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∫–µ—à–∞ –≤ `agent.py`

```python
from .cache import get_cache

def tzd_reader(
    files: List[str],
    engine: str = "gpt",
    base_dir: Optional[str] = None,
    use_cache: bool = True
) -> Dict[str, Any]:
    """Main function with caching"""
    
    cache = get_cache()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
    if use_cache:
        cached_result = cache.get(files, engine)
        if cached_result:
            logger.info("Using cached result")
            cached_result['from_cache'] = True
            cached_result['cache_timestamp'] = time.strftime('%Y-%m-%d %H:%M:%S')
            return cached_result
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑
    start_time = time.time()
    
    try:
        # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –ª–æ–≥–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ ...
        
        result = {
            # ... —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ ...
            'from_cache': False
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
        if use_cache:
            cache.set(files, engine, result)
        
        return result
        
    except Exception as e:
        logger.error(f"Analysis failed: {e}")
        raise
```

### API –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–µ—à–µ–º

```python
# app/routers/tzd_router.py
from agents.tzd_reader.cache import get_cache

@router.get("/cache/stats")
async def get_cache_stats():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–µ—à–∞"""
    cache = get_cache()
    return cache.get_stats()

@router.post("/cache/clear")
async def clear_cache():
    """–û—á–∏—Å—Ç–∫–∞ –∫–µ—à–∞"""
    cache = get_cache()
    cache.clear()
    return {"message": "Cache cleared successfully"}
```

---

## üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –£–ª—É—á—à–µ–Ω–∏–π

### –°–∫—Ä–∏–ø—Ç –¥–ª—è –Ω–∞–≥—Ä—É–∑–æ—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

```python
# tests/load_test.py
"""
–ù–∞–≥—Ä—É–∑–æ—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ TZDReader
"""
import asyncio
import time
from pathlib import Path
import statistics

async def test_concurrent_requests(file_path: str, num_requests: int = 10):
    """–¢–µ—Å—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    from agents.tzd_reader import TZDReader
    
    reader = TZDReader(engine="gpt")
    
    async def single_request():
        start = time.time()
        try:
            result = reader.analyze([file_path])
            return time.time() - start, True
        except Exception as e:
            return time.time() - start, False
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    tasks = [single_request() for _ in range(num_requests)]
    results = await asyncio.gather(*tasks)
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    durations = [r[0] for r in results]
    successes = [r[1] for r in results]
    
    print(f"\n=== Load Test Results ===")
    print(f"Total requests: {num_requests}")
    print(f"Successful: {sum(successes)}")
    print(f"Failed: {num_requests - sum(successes)}")
    print(f"\nDuration statistics:")
    print(f"  Min: {min(durations):.2f}s")
    print(f"  Max: {max(durations):.2f}s")
    print(f"  Mean: {statistics.mean(durations):.2f}s")
    print(f"  Median: {statistics.median(durations):.2f}s")


if __name__ == "__main__":
    test_file = "test_data/sample.pdf"
    asyncio.run(test_concurrent_requests(test_file, num_requests=20))
```

---

## ‚úÖ –ß–µ–∫–ª–∏—Å—Ç –í–Ω–µ–¥—Ä–µ–Ω–∏—è

### –§–∞–∑–∞ 1: –ë–∞–∑–æ–≤—ã–µ –£–ª—É—á—à–µ–Ω–∏—è (–ù–µ–¥–µ–ª—è 1)
- [ ] –°–æ–∑–¥–∞—Ç—å `exceptions.py` —Å –∏–µ—Ä–∞—Ä—Ö–∏–µ–π –∏—Å–∫–ª—é—á–µ–Ω–∏–π
- [ ] –û–±–Ω–æ–≤–∏—Ç—å `agent.py` –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–æ–≤—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π
- [ ] –û–±–Ω–æ–≤–∏—Ç—å `tzd_router.py` –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫
- [ ] –î–æ–±–∞–≤–∏—Ç—å retry-–ª–æ–≥–∏–∫—É —Å tenacity
- [ ] –ù–∞–ø–∏—Å–∞—Ç—å —Ç–µ—Å—Ç—ã –¥–ª—è –Ω–æ–≤—ã—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π

### –§–∞–∑–∞ 2: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ (–ù–µ–¥–µ–ª—è 2)
- [ ] –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å structlog –∏ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å `logging_config.py`
- [ ] –û–±–Ω–æ–≤–∏—Ç—å –≤—Å–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ
- [ ] –°–æ–∑–¥–∞—Ç—å `monitoring.py` —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ Prometheus
- [ ] –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –≤ `agent.py`
- [ ] –î–æ–±–∞–≤–∏—Ç—å `/metrics` —ç–Ω–¥–ø–æ–∏–Ω—Ç

### –§–∞–∑–∞ 3: –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–ù–µ–¥–µ–ª—è 3)
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–æ–≤
- [ ] –°–æ–∑–¥–∞—Ç—å —Å–∏—Å—Ç–µ–º—É –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è (`cache.py`)
- [ ] –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–µ—à –≤ `agent.py`
- [ ] –î–æ–±–∞–≤–∏—Ç—å API –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–µ—à–µ–º
- [ ] –ü—Ä–æ–≤–µ—Å—Ç–∏ –Ω–∞–≥—Ä—É–∑–æ—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### –§–∞–∑–∞ 4: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è (–ù–µ–¥–µ–ª—è 4)
- [ ] –û–±–Ω–æ–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
- [ ] –ù–∞–ø–∏—Å–∞—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã
- [ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å CI/CD –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–µ—Ç—Ä–∏–∫
- [ ] –ü—Ä–æ–≤–µ—Å—Ç–∏ –∫–æ–¥-—Ä–µ–≤—å—é
- [ ] –†–∞–∑–≤–µ—Ä–Ω—É—Ç—å –≤ production

---

## üìû –ü–æ–¥–¥–µ—Ä–∂–∫–∞

–ü—Ä–∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –æ–±—Ä–∞—â–∞–π—Ç–µ—Å—å:
- **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:** [TZD_ARCHITECTURE_REVIEW.md](./TZD_ARCHITECTURE_REVIEW.md)
- **Issues:** GitHub Issues
- **Logs:** `logs/tzd_reader.log`, `logs/tzd_reader_errors.log`

---

**–í–µ—Ä—Å–∏—è:** 1.0  
**–î–∞—Ç–∞:** 2024-01-10  
**–°—Ç–∞—Ç—É—Å:** Ready for Implementation
